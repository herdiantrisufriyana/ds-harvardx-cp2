---
title: 'CYO Project: Predicting pregnancy outcome by machine learning
  application on a nationwide health insurance dataset'
author: 'Herdiantri Sufriyana, Yu-Wei Wu, Emily Chia-Yu Su'
output: pdf_document
---

```{r Set sample kind, include=FALSE}
# sample.kind=NULL # if using R 3.5 or earlier
sample.kind='Rounding' # if using R 3.6 or later
```

```{r Set to run or not run very heavy computations, include=FALSE}
# Many computations were very heavy;
# thus, we provided the RDS files as subtitutes and load only ones
# that can be ran in most computers.
# Set to TRUE if you want to run the very heavy computations.
run_heavy_computation=FALSE
```

\tableofcontents
\newpage
\listoffigures
\listoftables
\newpage

# Introduction

```{r Install and set specific version of Bioconductor, include=FALSE}
# Install devtools to install specific version of BiocManager
if(!require(devtools)) install.packages('devtools')

# Install specific version of BiocManager and Bioconductor
if(!require(BiocManager) | packageVersion('BiocManager')!='1.30.10'){
  devtools::install_version('BiocManager',version='1.30.10')
}
if(version()!='3.11') install(version='3.11',update=TRUE,ask=FALSE)

# Unload devtools & BiocManager to prevent overlapped functions with others
detach('package:devtools',unload=T)
detach('package:BiocManager',unload=T)
```

```{r Install specific packages with specific Bioconductor,include=FALSE}
BiocManager::install(
  c(
    'tidyverse','lubridate','dslabs','pbapply','broom','caret','igraph'
    ,'data.table','kableExtra','zeallot','doParallel','WCGNA','MLeval','glmnet'
    ,'Rborist','lda'
  ),
  update=F
)
```

```{r Load packages, include=FALSE}
library(tidyverse)
options(dplyr.summarise.inform=FALSE)
dslabs::ds_theme_set()
library(parallel)
library(pbapply)
library(lubridate)
library(broom)
library(caret)
library(igraph)
library(data.table)
library(kableExtra)
library(zeallot)
library(doParallel)
library(WGCNA)
library(MLeval)
library(gbm)
```

## Dataset and variables

```{r Load preselected data, include=FALSE}
selection=readRDS('data/selection.rds')
target_population=readRDS('data/target_population.rds')
```

The dataset was taken from a nationwide health insurance dataset consisting
~1.6 million patients. Although we used deidentified dataset, it's not possible
to share the dataset; thus, we selected and recoded the dataset in order to hide
the data source information (Table 1). For research confidentiality, we cannot
report the diagnosis being the predicted outcome. Instead, we will only report
the diagnosis as 'outcome'. The outcome was a particular pregnancy endpoint. We
also cannot mention the references for the same reason.

```{r echo=FALSE}
selection %>%
  mutate(step=str_to_sentence(step)) %>%
  `colnames<-`(c(
    'Selection criterion'
    ,'Excluded visits'
    ,'Included visits'
    ,'Excluded subjects'
    ,'Included subjects'
  )) %>%
  knitr::kable(
    format='latex'
    ,caption='Instance selection'
    ,format.args=list(big.mark=',')
  ) %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

We selected 12-to-55-years-old pregnant women with any visits to any healthcare
providers nationwide. The dataset consisted of 2-year records, but we only
selected any records of a patient up to 1 day before the earliest date of event
visit or the latest pregnancy visit date for the non-event visit. Event is
defined if a visit was done by a subject encountered with a diagnosis code
of interest. If a subject is not encountered by this code, then her visits
were considered as non event. Therefore, we used visits as instances (n=114,133)
for developing a prediction model using predictors derived from 65 attributes.

## Goal

The model predicts whether a patient will be encountered with the diagnosis code
of interest. The prediction is conducted everytime she visits a healthcare
provider and gets recorded into the medical database. This is why we chose up
to 1 day before the event. If being predicted as event, then it may happen a day
after at the earliest. Since the outcome is theoretically happening in a very
specific period within pregnancy, a clinician can contextually interpret when
the outcome probably happens.

## Key steps

To achieve the goal of the this project, we conducted these key steps:

1. Data preprocessing (data cleaning)
2. Data exploration and visualization
3. Any insights gained
4. Modeling approach
   - Model development
   - Model calibration
   - Model evaluation
   - Model validation


# Methods

## Data preprocessing

First, we filtered 65 attributes that has only completed data. There were
only 26 attributes that fulfill this criterion (Table 2). Briefly, the
attributes covered information regarding identity codes, subject demography
(age, occupation, insurance class, and marital status), healthcare provider
information, diagnosis/procedure ICD-10 codes, sampling-related attributes, and
outcome-related attributes.

```{r echo=FALSE}
readRDS('data/target_population.rds') %>%
  
  # Join all codes of diagnosis/procedure
  left_join(readRDS('data/admission_diagnosis.rds'),by='visit_id') %>%
  left_join(readRDS('data/secondary_diagnoses.rds'),by='visit_id') %>%
  left_join(readRDS('data/procedures.rds'),by='visit_id') %>%
  
  # Assign 0 if missing
  mutate_all(function(x)!is.na(x)) %>%
  mutate_all(as.integer) %>%
  
  # Convert to matrix and reverse the row order
  t(.) %>%
  .[nrow(.):1,] %>%
  
  # Get proportion of non-missing values for each attribute
  lapply(X=1,Y=.,function(X,Y){
    data.frame(non_missing_p=rowMeans(Y))
  }) %>%
  .[[1]] %>%
  
  # Filter completed attributes
  filter(non_missing_p==1) %>%
  rownames() %>%
  
  # Create a table to show the completed attributes classified as reported
  data.frame(Attributes=.) %>%
  mutate(
    Group=case_when(
      str_detect(Attributes,'id')
      ~'1. Identity codes'
      ,str_detect(Attributes,'healthcare|reghc')
       ~'3. Healthcare provider information'
      ,str_detect(Attributes,'icd10')
       ~'3. Diagnosis/procedure'
      ,str_detect(Attributes,'sampl')
       ~'4. Sampling-related attributes'
      ,str_detect(Attributes,'outcome|event')
       ~'5. Outcome-related attributes'
      ,TRUE
       ~'2. Subject demography'
    )
  ) %>%
  arrange(Group,Attributes) %>%
  group_by(Group) %>%
  summarize(Attributes=paste(Attributes,collapse=', ')) %>%
  knitr::kable(format='latex',caption='Non-missing attributes') %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

Then, we spread the ICD-10 codes whereas a code had a column. If a visit was
encountered by a code, then value for the code column was 1; otherwise, the
value was 0. Then, we computed the number of days each code was encountered for
a subject up to a visit. For example, if a code is encountered twice in a day,
we just record the code having value of 1. If a code is ever encountered for
three days before a visit, then we record the code having value of 3 for that
visit. Therefore, we count frequency of each code being encountered but only
counting it once for each day. This would be medical histories for predictors.
We constructed 2 types of table for medical history. 

```{r Create medical history by code encounter for each visit, include=FALSE}
if(run_heavy_computation){
  
  medical_history=
    
    # Join all codes of diagnosis/procedure
    suppressWarnings(separate(
      readRDS('data/target_population.rds') %>%
        mutate(seq=seq(nrow(.))) %>%
        left_join(readRDS('data/admission_diagnosis.rds'),by='visit_id') %>%
        left_join(readRDS('data/secondary_diagnoses.rds'),by='visit_id') %>%
        left_join(readRDS('data/procedures.rds'),by='visit_id')
      ,icd9_code_desc,c('icd9_code','icd9_desc'),sep='\\s?-\\s?'
    )) %>%
    
    # Differentiate if a provider is primary care
    mutate(
      est_strata_id=
        est_strata_id %>%
        paste0('.',ifelse(str_detect(insurance_model,'Primary care'),1,2))
    ) %>%
    
    # Select diagnosis/procedure-related attributes only
    select(
      subject_id
      ,day_to_event
      ,est_strata_id
      ,icd10_code
      ,admission_icd10_code
      ,secondary_icd10_code
      ,icd9_code
      ,seq
    ) %>%
    
    # Pivot longer the codes
    pivot_longer(
      colnames(.) %>%
        .[!.%in%c('subject_id','day_to_event','est_strata_id','seq')]
      ,names_to='code_type'
      ,values_to='code'
    ) %>%
    
    # Give a sign if the pivoted code column is missing.
    # This is because not all visits have a particular type of code
    mutate(
      code=ifelse(is.na(code)|code=='','NA',code)
      ,value=ifelse(code=='NA',0,1)
    )
  
}
```

```{r Build a function to unevenly split a vector given a length, include=FALSE}
split_len=function(x,len){
  split_idx=round(seq(1,length(x),len=len))
  lapply(X=1:(length(split_idx)-1),Y=split_idx,Z=x,function(X,Y,Z){
    Z[Y[X]:(Y[X+1]-ifelse(X==(length(split_idx)-1),0,1))]
  })
}
```

First, we count frequency of a code being encountered for each subject up to
each visit. But we count the frequency without separating healthcare providers
which a subject visits to. We called this table as nationwide medical history.
We used this table to identify causal factors. In this table, no medical history
is censored since we know the medical history of a patient across all providers.

```{r Compute nationwide cumulated code encounter count of ..., include=FALSE}
if(run_heavy_computation){
  cat('Compute nationwide cumulated encounter count of a code for',append=T)
  cat('each subject every visit\n')
  cat('Started:',as.character(now()),'\n')
  cl=makeCluster(detectCores()-1)
  clusterEvalQ(cl,{
    library('tidyverse')
    library('pbapply')
  })
    
    mh_nationwide=
      
      # Compute the number of days a code was encountered for a subject
      # up to each visit from any kind of codes and any healthcare providers
      medical_history %>%
      group_by(subject_id,day_to_event,code,seq) %>%
      summarize(
        d_enc_adm=sum(ifelse(code_type=='admission_icd10_code',value,0))
        ,d_enc_dur=sum(ifelse(code_type=='secondary_icd10_code',value,0))
        ,d_enc_dis=sum(ifelse(code_type=='icd10_code',value,0))
        ,p_enc=sum(ifelse(code_type=='icd9_code',value,0))
      ) %>%
      ungroup() %>%
      mutate(
        any_enc=as.integer(any(c(d_enc_adm>0,d_enc_dur>0,d_enc_dis>0,p_enc>0)))
      ) %>%
      select(-d_enc_adm,-d_enc_dur,-d_enc_dis,-p_enc) %>%
      spread(code,any_enc,fill=0) %>%
      select(-seq,-`NA`) %>%
      arrange(subject_id,desc(day_to_event))
    gc()
    
    mh_nationwide=
      
      # Compute cumulated number of the days for each code
      mh_nationwide %>% 
      .$subject_id %>%
      .[!duplicated(.)] %>%
      split_len(len=500) %>%
      pblapply(X=seq(length(.)),Y=.,Z=mh_nationwide,cl=cl,function(X,Y,Z){
        filter(Z,subject_id%in%Y[[X]]) %>%
          group_by(subject_id) %>%
          mutate_at(
            colnames(.) %>% .[!.%in%c('subject_id','day_to_event')]
            ,cumsum
          ) %>%
          ungroup()
      }) %>%
      do.call(rbind,.)

  stopCluster(cl)
  rm(cl)
  gc()
  cat('End:',as.character(now()))
  saveRDS(mh_nationwide,'data/mh_nationwide.rds')
}else{
  cat(readRDS('data/log.rds')[['mh_nationwide']])
}
```

Second, we count the frequency with separating healthcare providers which a
subject visits to. We called this table as provider medical history. We used
this table to develop predictions models. This reflects real-world data for
deploying the models since a healthcare provider is unlikely to access medical
record of a patient in other providers. Therefore, we need a prediction model
that only use patient data in each provider.

```{r Compute cumulated encounter ... by provider database, include=FALSE}
if(run_heavy_computation){
  cat('Compute cumulated encounter count of a code for ',append=T)
  cat('each subject every visit isolated by provider database\n')
  cat('Started:',as.character(now()),'\n')
  cl=makeCluster(detectCores()-1)
  clusterEvalQ(cl,{
    library('tidyverse')
    library('pbapply')
  })
    
    mh_provider=
      
      # Compute the number of days a code was encountered for a subject
      # up to each visit from any kind of codes to each healthcare provider
      medical_history %>%
      group_by(subject_id,day_to_event,est_strata_id,code,seq) %>%
      summarize(
        d_enc_adm=sum(ifelse(code_type=='admission_icd10_code',value,0))
        ,d_enc_dur=sum(ifelse(code_type=='secondary_icd10_code',value,0))
        ,d_enc_dis=sum(ifelse(code_type=='icd10_code',value,0))
        ,p_enc=sum(ifelse(code_type=='icd9_code',value,0))
      ) %>%
      ungroup() %>%
      mutate(
        any_enc=as.integer(any(c(d_enc_adm>0,d_enc_dur>0,d_enc_dis>0,p_enc>0)))
      ) %>%
      select(-d_enc_adm,-d_enc_dur,-d_enc_dis,-p_enc) %>%
      spread(code,any_enc,fill=0) %>%
      select(-seq,-`NA`) %>%
      arrange(subject_id,desc(day_to_event),est_strata_id)
    gc()
    
    mh_provider=
      
      # Compute cumulated number of the days for each code
      mh_provider %>% 
      .$subject_id %>%
      .[!duplicated(.)] %>%
      split_len(len=500) %>%
      pblapply(X=seq(length(.)),Y=.,Z=mh_provider,cl=cl,function(X,Y,Z){
        filter(Z,subject_id%in%Y[[X]]) %>%
          group_by(subject_id,est_strata_id) %>%
          mutate_at(
            colnames(.) %>% .[!.%in%c('subject_id','day_to_event')]
            ,cumsum
          ) %>%
          ungroup()
      }) %>%
      do.call(rbind,.)

  stopCluster(cl)
  rm(cl)
  gc()
  cat('End:',as.character(now()))
  saveRDS(mh_provider,'data/mh_provider.rds')
}else{
  cat(readRDS('data/log.rds')[['mh_provider']])
}
```

We have conducted data wrangling by pivoting the dataset longer and wider
multiple times. This may incidentally cause unexpected data attrition. We need
to check data integrity by comparing the numbers of event and non event
(Table 3).

```{r Sanity check for the number of visits and subjects, include=FALSE}
if(run_heavy_computation){
  rbind(
    
    # Original dataset
    readRDS('data/target_population.rds') %>%
      lapply(X=1:2,Y=.,function(X,Y){
        
        # Visit data
        if(X==1){
          group_by(Y,outcome) %>%
            summarize(n=n()) %>%
            spread(outcome,n) %>%
            mutate(total=event+`non-event`) %>%
            mutate(type='visit')
        
        # Subject data
        }else{
          select(Y,subject_id,outcome) %>%
            .[!duplicated(.),] %>%
            group_by(outcome) %>%
            summarize(n=n()) %>%
            spread(outcome,n) %>%
            mutate(total=event+`non-event`) %>%
            mutate(type='subject')
        }
      }) %>%
      do.call(rbind,.) %>%
      mutate(dataset='original')
    
    # Nationwide medical history
    ,mh_nationwide %>%
      lapply(X=1:2,Y=.,function(X,Y){
        
        # Visit data
        if(X==1){
          Y %>%
            left_join(
              target_population %>%
                select(subject_id,outcome) %>%
                .[!duplicated(.),]
              ,by='subject_id'
            ) %>%
            group_by(outcome) %>%
            summarize(n=n()) %>%
            spread(outcome,n) %>%
            mutate(total=event+`non-event`) %>%
            mutate(type='visit')
        
        # Subject data
        }else{
          Y %>%
            left_join(
              target_population %>%
                select(subject_id,outcome) %>%
                .[!duplicated(.),]
              ,by='subject_id'
            ) %>%
            select(subject_id,outcome) %>%
            .[!duplicated(.),] %>%
            group_by(outcome) %>%
            summarize(n=n()) %>%
            spread(outcome,n) %>%
            mutate(total=event+`non-event`) %>%
            mutate(type='subject')
        }
      }) %>%
      do.call(rbind,.) %>%
      mutate(dataset='nationwide')
    
    # Provider medical history
    ,mh_provider %>%
      lapply(X=1:2,Y=.,function(X,Y){
        
        # Visit data
        if(X==1){
          Y %>%
            left_join(
              target_population %>%
                select(subject_id,outcome) %>%
                .[!duplicated(.),]
              ,by='subject_id'
            ) %>%
            group_by(outcome) %>%
            summarize(n=n()) %>%
            spread(outcome,n) %>%
            mutate(total=event+`non-event`) %>%
            mutate(type='visit')
        
        # Subject data
        }else{
          Y %>%
            left_join(
              target_population %>%
                select(subject_id,outcome) %>%
                .[!duplicated(.),]
              ,by='subject_id'
            ) %>%
            select(subject_id,outcome) %>%
            .[!duplicated(.),] %>%
            group_by(outcome) %>%
            summarize(n=n()) %>%
            spread(outcome,n) %>%
            mutate(total=event+`non-event`) %>%
            mutate(type='subject')
        }
      }) %>%
      do.call(rbind,.) %>%
      mutate(dataset='provider')
  ) %>%
  saveRDS('data/sanity_check.rds')
}
```

```{r echo=FALSE}
readRDS('data/sanity_check.rds') %>%
  mutate_at(c('type','dataset'),str_to_sentence) %>%
  `colnames<-`(str_to_sentence(colnames(.))) %>%
  kable(
    format='latex'
    ,caption='Data integrity'
    ,format.args=list(big.mark=',')
  ) %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

Data partition was conducted to get internal and external validation sets for
training and testing the prediction models. We also explored the data using only
internal validation set. This is intended to get better generalization of the
data interpretation and the prediction models. To get external validation set,
we applied either geographical or temporal split, as recommended by PROBAST
guidelines. We also overlapped both splits to get third partition which is
geotemporal split.

For geographical split, we randomly excluded a list of cities where a healthcare
provider which a subject was registered to (not a subject visit to). Unlike
a visiting provider, a registering provider is only one for each subject. We
tried several numbers of city proportion for each province to exclude. We
excluded visits of a subject that was registered in a healthcare provider in
these cities. We excluded cities for each province to avoid racial differences.
Some provinces have a dominant race over others. This factor affects the outcome
based on previous studies. Although this challenge may show robustness of a
prediction model, race may also increase the predictive performance that may
make us too optimist.

For geotemporal split, of the excluded cities, we also tried several numbers of
the proportion to exclude for the second time. From this exclusion, we
overlapped the visits with those from temporal split. We chose the proportions
of exclusion in order to get total excluded visits as much as we need from any
splitting methods. Second exclusion was needed since the geographical split
tradeoff the size of the geotemporal split. Meanwhile, we need to achieve
a minimum number of events for each partition of external validation set. We
will explain the external validation size requirement after description of
temporal splitting below.

```{r Exclude sufficient-sample city for ext. validation, echo=FALSE}
suppressWarnings(set.seed(33,sample.kind=sample.kind))
extv_city=
  
  # Group visits by city of healthcare provider
  # of which a subject registered to (not a subject visit to)
  target_population %>%
  group_by(reghc_province,reghc_city,subject_id) %>%
  summarize(
    event=as.integer(sum(ifelse(outcome=='event',1,0))>0)
    ,non_event=as.integer(sum(ifelse(outcome=='non-event',1,0))>0)
  ) %>%
  
  # Compute the prevalence of event in each of the cities
  group_by(reghc_province,reghc_city) %>%
  summarize(
    event=sum(event)
    ,non_event=sum(non_event)
  ) %>%
  ungroup() %>%
  mutate(prevalence=event/(event+non_event)) %>%
  
  # Exclude cities and show the prevalence
  lapply(X=1,Y=.,function(X,Y){
    
    # Randomly exclude cities
    ## Filter cities of which event prevalence is between 0 and 1.
    ## Then, sample 22.5% cities for each province without replacement
    Z=filter(Y,prevalence>0 & prevalence<1) %>%
      group_by(reghc_province) %>%
      summarize(extv_city=sample(reghc_city,round(n()*0.225),F))
    
    ## From the sampled cities, sample 40% to overlap with
    ## those of temporal split.
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    Z %>%
      group_by(reghc_province) %>%
      summarize(extv_city=sample(extv_city,round(n()*0.4),F)) %>%
      ungroup() %>%
      mutate(bgt_city=1) %>%
      right_join(Z,by=c('reghc_province','extv_city')) %>%
      mutate(bgt_city=ifelse(is.na(bgt_city),0,1)) %>%
      arrange(reghc_province,extv_city)
    
  }) %>%
  .[[1]]
```

For temporal split, we randomly exclude a period consisting a particular number
of days for each season. Like racial difference, season affects the outcome
based on previous studies. To avoid overoptimism, we excluded a period for
each season.

```{r Exclude time-outcome-independent period for ext. validation, echo=FALSE}
suppressWarnings(set.seed(33,sample.kind=sample.kind))
extv_date=
  
  # Group visits by outcome and month of the event
  target_population %>%
  select(subject_id,event_t,outcome) %>%
  .[!duplicated(.),] %>%
  mutate(month_t=round_date(event_t,unit='month')) %>%
  mutate(x=round((month_t-as_date('2015-01-01'))/dmonths(1))) %>%
  group_by(outcome,month_t,x) %>%
  summarize(subject=n()) %>%
  ungroup() %>%
  
  # Create variable season
  mutate(
    season=case_when(
      between(month_t,as_date('2015-01-01'),as_date('2015-03-20'))~'winter'
      ,between(month_t,as_date('2015-03-21'),as_date('2015-06-21'))~'spring'
      ,between(month_t,as_date('2015-06-22'),as_date('2015-09-23'))~'summer'
      ,between(month_t,as_date('2015-09-24'),as_date('2015-12-22'))~'autumn'
      ,between(month_t,as_date('2015-12-23'),as_date('2016-03-20'))~'winter'
      ,between(month_t,as_date('2016-03-21'),as_date('2016-06-20'))~'spring'
      ,between(month_t,as_date('2016-06-21'),as_date('2016-09-22'))~'summer'
      ,between(month_t,as_date('2016-09-23'),as_date('2016-12-31'))~'autumn'
      ,TRUE~'none'
    )
  ) %>%
  
  # Make sure only including visits on 2015 and 2016
  mutate(year_t=substr(as.character(month_t),1,4)) %>%
  filter(year_t%in%2015:2016) %>%
  
  # For each season, sample a 30-day window to exclude
  lapply(X=seq(sum(!duplicated(paste0(.$year_t,'_',.$season))))
         ,Y=paste0(.$year_t,'_',.$season) %>% .[!duplicated(.)]
         ,Z=.
         ,function(X,Y,Z){
    K=Z %>%
      filter(
        year_t==str_split_fixed(Y[X],'_',2)[,1] &
        season==str_split_fixed(Y[X],'_',2)[,2]
      ) %>%
      mutate(
        min_date=min(month_t)
        ,max_date=ceiling_date(max(month_t),unit='month')-ddays(1)
      )
    L=sample(as_date(K$min_date[1]:K$max_date[1]),1,F)
    
    K %>%
      mutate(min_date=L-days(15),max_date=L+days(15)) %>%
      .[!duplicated(.),]
  }) %>%
  do.call(rbind,.) %>%
  
  # Randomly exclude time-outcome-independent period 
  select(year_t,season,min_date,max_date) %>%
  .[!duplicated(.),] %>%
  filter(!((year_t=='2015' & season=='winter')|
           (year_t=='2016' & season=='autumn')))
```

After getting the list of excluded cities and event periods, we conducted data
partition. First, we excluded visits from subjects registered to providers in
the excluded cities. Second, we excluded visits from subjects with event date in
the excluded period. For geotemporal split, geographically-excluded visits were
filtered with those of secondly excluded cities and those with event in the
excluded periods. For internal validation set, data were held out from any of
the splits. For geographical and temporal split, we subtracted geotemporal split
from each split. We also split the internal validation set applying k-fold
cross validation.

```{r Conduct data partition for int. and ext. validation, echo=FALSE}
if(run_heavy_computation){
  set=list()
  
  # Select attributes needed for partition
  set$intv=
    target_population %>%
    select(subject_id,outcome,reghc_city,event_t) %>%
    .[!duplicated(.),] %>%
    mutate(outcome=factor(outcome,c('non-event','event')))
  
  # Exclude visits from subjects registered to providers in the excluded cities
  # for geographical split
  set$extv_geo=
    set$intv %>%
    filter(reghc_city %in% extv_city$extv_city)
  
  # Exclude visits from subjects with event date in the excluded period
  # for temporal split
  set$extv_tem=
    set$intv %>%
    lapply(X=1:nrow(extv_date),Y=extv_date,Z=.,function(X,Y,Z){
      filter(Z,between(event_t,Y$min_date[X],Y$max_date[X]))
    }) %>%
    do.call(rbind,.)
  
  # Filter geographically-excluded visits with those of secondly excluded cities
  # and those with event in the excluded period
  # for geotemporal split
  set$extv_bgt=
    set$extv_geo %>%
    lapply(X=1:nrow(extv_date),Y=extv_date,Z=.,function(X,Y,Z){
      Z %>%
        filter(
          reghc_city %in% filter(extv_city,bgt_city==1)$extv_city &
          between(event_t,Y$min_date[X],Y$max_date[X])
        )
    }) %>%
    do.call(rbind,.)
  
  # Hold out the data from any splits
  # for internal validation set
  set$intv=
    set$intv %>%
    filter(!((reghc_city %in% set$extv_geo$reghc_city)|
             (event_t %in% set$extv_tem$event_t)))
  
  # From geographical split, subtract geotemporal split
  set$extv_geo=
    set$extv_geo %>%
    filter(!(reghc_city %in% set$extv_bgt$reghc_city))
  
  # From temporal split, subtract geotemporal split
  set$extv_tem=
    set$extv_tem %>%
    filter(!(event_t %in% set$extv_bgt$event_t))
  
  # From internal validation set, get 5 of 6 parts for training
  # the remaining for validation.
  # This will be cross validation set.
  suppressWarnings(set.seed(33,sample.kind=sample.kind))
  set$cv_idx=
    createDataPartition(set$intv$outcome,times=6,p=5/6,list=F) %>%
    lapply(X=1:ncol(.),Y=.,Z=set$intv,function(X,Y,Z){
      train_set=Z[!seq(nrow(Z))%in%Y[,X],]
      test_set=
        Z[Y[,X],] %>%
        filter(!((reghc_city %in% train_set$reghc_city)|
                 (event_t %in% train_set$event_t)))
      test_set$subject_id
    })
  
  # Summarize prevalence of interval validation set
  set$stats[[1]]=
    data.frame(
      set_subject='Internal validation, cross validation'
      ,event=NA
      ,non_event=NA
      ,prevalence=
        sapply(set$cv_idx,function(x){
          outcome=filter(set$intv,subject_id %in% x)$outcome
          sum(outcome=='event')/length(outcome)
        })
    ) %>%
    group_by(set_subject,event,non_event) %>%
    summarize(
      lb=mean(prevalence)-qnorm(0.975)*sd(prevalence)/sqrt(n())
      ,ub=mean(prevalence)+qnorm(0.975)*sd(prevalence)/sqrt(n())
      ,prevalence=mean(prevalence)
    ) %>%
    ungroup() %>%
    select(set_subject,event,non_event,prevalence,lb,ub)
  
  # Summarize prevalences of external validation sets
  set$stats[[2]]=
    lapply(X=1:4,Y=set,function(X,Y){
      group_by(Y[[X]],outcome) %>%
        summarize(
          set_subject=
            c('Internal validation'
              ,paste('External validation,',c('geographical'
                                              ,'temporal'
                                              ,'geotemporal'),'split')
              )[X]
          ,n=n()
        ) %>%
        spread(outcome,n) %>%
        ungroup() %>%
        rename(non_event=`non-event`) %>%
        mutate(prevalence=event/(event+non_event)) %>%
        mutate(lb=NA,ub=NA)
    }) %>%
    do.call(rbind,.)
  
  # Summarize prevalences by subject
  set$stats[['subject']]=
    set$stats %>%
    do.call(rbind,.) %>%
    .[c(2,1,3:5),]
  
  # Summarize visits by validation set
  set$stats[['visit']]=
    lapply(
      X=1:3
      ,Y=c(paste('External validation,',c('geographical'
                                           ,'temporal'
                                           ,'geotemporal'),'split')
           )
      ,Z=c(paste0('extv_',c('geo','tem','bgt')))
      ,FUN=function(X,Y,Z){
        data.frame(
          set_visit=Y[X]
          ,event=sum(
            mh_nationwide$subject_id %in%
              set[[Z[X]]]$subject_id[set[[Z[X]]]$outcome=='event']
          )
          ,non_event=sum(
            mh_nationwide$subject_id %in%
              set[[Z[X]]]$subject_id[set[[Z[X]]]$outcome=='non-event']
          )
        )
    }) %>%
    do.call(rbind,.) %>%
    
    # Add row for total visit of external validation sets
    add_row(
      set_visit='External validation'
      ,event=
        sum(filter(.,str_detect(set_visit,'External validation'))$event)
      ,non_event=
        sum(filter(.,str_detect(set_visit,'External validation'))$non_event)
    ) %>%
    
    filter(set_visit!='Internal validation') %>%
    
    # Add row for visit of internal validation set
    add_row(
      set_visit='Internal validation'
      ,event=
        (sum(target_population$outcome=='event')-
         filter(.,set_visit=='External validation')$event)
      ,non_event=
        (sum(target_population$outcome=='non-event')-
         filter(.,set_visit=='External validation')$non_event)
    ) %>%
    
    # Add row for visit of internal validation set, training split
    add_row(
      set_visit='Internal validation, training split'
      ,event=
        round(5/6*filter(.,set_visit=='Internal validation')$event)
      ,non_event=
        round(5/6*filter(.,set_visit=='Internal validation')$non_event)
    ) %>%
    
    # Add row for visit of internal validation set, validation split
    add_row(
      set_visit='Internal validation, validation split'
      ,event=
        filter(.,set_visit=='Internal validation')$event-
        filter(.,set_visit=='Internal validation, training split')$event
      ,non_event=
        filter(.,set_visit=='Internal validation')$non_event-
        filter(.,set_visit=='Internal validation, training split')$non_event
    ) %>%
    
    # Compute proportion of each set
    mutate(
      subtotal=event+non_event
      ,total=nrow(target_population)
    ) %>%
    mutate(
      p=subtotal/total
    ) %>%
  
  # Show the number and proportion of data partition
  arrange(factor(set_visit,c(
    'Internal validation'
    ,'Internal validation, training split'
    ,'Internal validation, validation split'
    ,'External validation'
    ,'External validation, geographical split'
    ,'External validation, temporal split'
    ,'External validation, geotemporal split'
  )))
  
  set$stats[1:2]=NULL
  
  saveRDS(set,'data/set.rds')
}else{
  set=readRDS('data/set.rds')
}
```

Based on PROBAST guidelines, we pursued >100 event for each split of external
validation (Table 4). We considered to get visits of validation split in
internal validation set, being similar to those of external validation sets.
Meanwhile, we need to keep >100 event; thus, several numbers of proportion were
found to fulfill those criteria. For each province, we found 22.5% cities being
able to exclude for geographical split, of which, 40% cities of them being able
to exclude for geotemporal split. We also found 30 days for each season being
able to exclude for temporal split. By ratio of 5:1:1 for
training:validation:testing, we could fulfill the partition criteria. By ratio
of 5:1, we could apply k-fold cross validation by k=6. This means our testing
sets (external validation) comprising ~15% of our data, which approach ~20%, the
rule of thumb of testing proportion that we also tried to fulfill. We expected
these numbers being able to estimate the generalizability of our prediction
models.

```{r echo=FALSE}
set$stats[['visit']] %>%
  select(-total) %>%
  mutate(p=round(p*100,2)) %>%
  mutate(
    set_visit=str_replace_all(set_visit,'Internal','Int.')
    ,set_visit=str_replace_all(set_visit,'External','Ext.')
  ) %>%
  setNames(c('Set','Event','Non-event','Total','Proportion (%)')) %>%
  knitr::kable(
    format='latex'
    ,caption='Data partition'
    ,format.args=list(big.mark=',')
  ) %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

## Data exploration and visualization

After data wrangling to get medical history, we could identify predictor
candidates. There were 2,966 predictors consisting medical histories as
either diagnosis or procedure ICD-10 codes, and non-medical history predictors.
Of the last type, the categorical predictors were counted as binary predictor.
This means a categorical predictor with 3 categories being counted as 3
predictors.

```{r Create predictors table for codes and the descriptions, include=FALSE}
if(run_heavy_computation){
  predictors=list()
  
  # Get column names form nationwide medical history
  predictors[[1]]=data.frame(predictor=colnames(mh_nationwide)[-1:-2])
  
  # Overlap the names with code description in discharge (primary) diagnosis
  predictors[[2]]=
    predictors[[1]] %>%
    inner_join(
      readRDS('data/target_population.rds') %>%
        select(icd10_code,icd10_3mer_desc) %>%
        setNames(c('predictor','description')) %>%
        .[!duplicated(.),]
      ,by='predictor'
    )
  
  # Overlap the names with code description in admission diagnosis
  predictors[[3]]=
    predictors[[1]] %>%
    inner_join(
      readRDS('data/admission_diagnosis.rds') %>%
        select(admission_icd10_code,admission_icd10_3mer_desc) %>%
        setNames(c('predictor','description')) %>%
        .[!duplicated(.),]
      ,by='predictor'
    )
  
  # Overlap the names with code description in discharge (secondary) diagnosis
  predictors[[4]]=
    predictors[[1]] %>%
    inner_join(
      readRDS('data/secondary_diagnoses.rds') %>%
        select(secondary_icd10_code,secondary_icd10_3mer_desc) %>%
        setNames(c('predictor','description')) %>%
        .[!duplicated(.),]
      ,by='predictor'
    )
  
  # Overlap the names with code description in procedure
  predictors[[5]]=
    predictors[[1]] %>%
    inner_join(
      suppressWarnings(separate(
          readRDS('data/procedures.rds') %>%
            select(icd9_code_desc) %>%
            .[!duplicated(.),,drop=F]
          ,icd9_code_desc,c('predictor','description')
          ,sep=' - '
        ))
      ,by='predictor'
    )
  
  # Get the overlaps only
  predictors=
    predictors[2:5] %>%
    do.call(rbind,.) %>%
    .[!duplicated(.),] %>%
    arrange(predictor)
  
  # Add non-medical history predictors
  predictors=
    readRDS('data/target_population.rds') %>%
    select(
      marital_status
      ,insurance_class
      ,occupation_segment
      ,healthcare_class
    ) %>%
    gather(predictor,value) %>%
    .[!duplicated(.),] %>%
    filter(!is.na(value)) %>%
    mutate(value=str_replace_all(value,'[:punct:]|[:space:]','_')) %>%
    unite(predictor,predictor,value,sep='.') %>%
    mutate(
      description=
        sapply(
          predictor
          ,function(x)paste(str_split(x,'_|\\.')[[1]],collapse=' ')
        )
    ) %>%
    rbind(data.frame(predictor='age',description='age')) %>%
    rbind(predictors)
  
  saveRDS(predictors,'data/predictors.rds')
}else{
  predictors=readRDS('data/predictors.rds')
}
```

To get insight of which predictors are important, we firstly explored from
previous studies to identify any factors that were correlated with the outcome.
By theoretical explanation proposed by previous studies, we made causal diagram
for the outcome. We merged the diagram, which is a directed acyclic graph (DAG),
from all causal factors being ever proposed for the outcome (Figure 1). To make
this diagram, we search for any causal factors (A) of the outcome (Y) and the
confounding factors (L). We found 28 proposed causal factors with 10 confounding
factors. For research confidentiality, we cannot disclose the previous studies
we considered to make the causal diagram.

```{r Load DAG list for causal diagram and inference, include=FALSE}
dag=readRDS('data/dag.rds')
```

```{r figure-1, echo=FALSE, fig.cap='Merged causal diagram', fig.height=10.5, fig.width=10.5}
plot_dag=function(edge_table
                  ,curved=1
                  ,root=c()
                  ,mode='in'
                  ,circular=F
                  ,plot.title=NULL){
  suppressWarnings(set.seed(66,sample.kind=sample.kind))
  lapply(X=1,Y=edge_table,function(X,Y){
      Z=graph_from_data_frame(directed=T,d=Y)
      E(Z)$curved=
        ifelse(substr(Y$from,1,1)=='A' & substr(Y$to,1,1)=='A',curved*(-1),0)
      E(Z)$width=
        ifelse(substr(Y$from,1,1)=='A' & substr(Y$to,1,1)=='A',2,1)
      V(Z)$color=
        unlist(mapply(
          RColorBrewer::brewer.pal
          ,RColorBrewer::brewer.pal.info %>%
            .[.$category=='qual',] %>%
            .$maxcolors
          ,RColorBrewer::brewer.pal.info %>%
            .[.$category=='qual',] %>%
            rownames()
        )) %>%
        .[as.integer(substr(names(V(Z)),2,3))]
      E(Z)$color=
        left_join(Y,data.frame(from=names(V(Z)),col=V(Z)$color),by='from')$col
      Z
    }) %>%
    .[[1]] %>%
    plot.igraph(
      layout=layout_as_tree(.,root=root,mode=mode,circular=circular)[,2:1]
      ,vertex.shape='circle'
      ,vertex.size=7.5
      ,vertex.label.color='black'
      ,vertex.label.cex=0.75
      ,edge.arrow.size=0.75
    )
  title(plot.title)
}
par(bg='gray80')
dag$baseline_edges %>%
  plot_dag(0,mode='in',circular=T,plot.title='')
```

Based on a causal diagram of each causal factor, we conducted G-estimation
to confirm the causal relationship using our data. For example, if we want to
confirm A02 as the causal factor, then we used the merged diagram to get edges
that are connected from and to A02 and the outcome or Y01 (Figure 2). To take
measurement error into account, we applied a measurement node. A factor is
measured by any of 1 or more diagnosis/procedure codes. If we did not have any
data to measure a factor, then we considered there is a backdoor path we cannot
adjust when conducting G-estimation for causal inference. In this example, a 
path of A02-A21-Y01 is considered as backdoor path since we cannot block the
confounding of A02-to-Y01 causal relationship. We also insert unmeasured nodes
(U) and assumed all of this measurement errors are independent non-differential.
Later for the causal model in G-estimation analysis, we included A20 and A25
with A02 as covariates and Y01 as the outcome. In the end, G-estimation would
determined whether A02 is causal factor, assuming the causal model is correct.

```{r figure-2, echo=FALSE, fig.cap='Example of A02 causal diagram', fig.height=10.5, fig.width=10.5}
backdoor_path=function(baseline_edge,measure_edge,causes,outcome){
  
  backdoor_pathway=function(edge_table,causes,outcome){
    lapply(causes,function(cause){
        edge_table %>%
          filter(
            (from==cause & to==outcome)
            |to==cause
            |(from%in%filter(.,to==cause)$from & to==outcome)
          )
      }) %>%
      do.call(rbind,.) %>%
      .[!duplicated(.),]
  }
  
  baseline_edge %>%
    backdoor_pathway(causes,outcome) %>%
    rbind(backdoor_pathway(measure_edge,c(.$from,.$to) %>% .[!duplicated(.)] %>% paste0('*'),paste0(outcome,'*')))
}
par(bg='gray80')
dag$baseline_edges %>%
  backdoor_path(dag$measure_edges,'A02','Y01') %>%
  plot_dag(0.2,'Y01','all',F)
```

To conduct G-estimation, we need to construct dataset for causal inference.
In addition to age and non-medical history categorical predictors, all of the
medical history predictors were zero if these are never encountered; otherwise,
this a probability from the number of days starting from 2 years before the
event. The inverse probability of the earliest medical history would be an
additional predictor which is censoring probability predictors. This indicated
how much day proportion of 2 years before the event, that is censored (a
medical  history is not recorded but probably existing).

```{r Conduct feature representation based on causal diagram, include=FALSE}
if(run_heavy_computation){
  
  cat('Conduct feature representation based on causal diagram\n')
  cat('Started:',as.character(now()),'\n')
  
  set$inference=
    
    # Standardize age, then normalize -1.96 to 1.96 into 0 to 1
    readRDS('data/target_population.rds') %>%
    select(subject_id,day_to_event,outcome,age) %>%
    .[!duplicated(.),] %>%
    filter(subject_id %in% set$intv$subject_id) %>%
    mutate(
      age_m=mean(age)
      ,age_s=sd(age)
      ,age=(qnorm(0.975)+(age-age_m)/age_s)/(2*qnorm(0.975))
    ) %>%
    
    # Add non-medical history categorical predictors
    left_join(
      
      # Group by subject and summarize the predictors
      readRDS('data/target_population.rds') %>%
        select(
          subject_id
          ,marital_status
          ,insurance_class
          ,occupation_segment
          ,healthcare_class
        ) %>%
        .[!duplicated(.),] %>%
        group_by(subject_id) %>%
        summarize_all(function(x){
          x[!duplicated(x)] %>%
            .[which.max(
              sapply(x[!duplicated(x)],function(x2)sum(x2==x,na.rm=T))
            )]
        }) %>%
        
        # Spread each categorical predictor as several binary predictors
        lapply(X=seq(ncol(.)),Y=.,function(X,Y){
          if(colnames(Y)[X] %in% c('marital_status'
                                   ,'insurance_class'
                                   ,'occupation_segment'
                                   ,'healthcare_class')){
            Y[,c('subject_id',colnames(Y)[X]),drop=F] %>%
              rename_at(colnames(Y)[X],function(x)'key') %>%
              mutate(
                key=str_replace_all(key,'[:punct:]|[:space:]','_')
                ,value=1
              ) %>%
              mutate_at('key',function(x)paste0(colnames(Y)[X],'.',x)) %>%
              spread(key,value) %>%
              select(-subject_id)
          }else{
            Y[,colnames(Y)[X],drop=F]
          }
        }) %>%
        do.call(cbind,.) %>%
        
        # If a category was applied (non-missing), then 1; otherwise 0
        mutate_at(
          colnames(.) %>% .[.!='subject_id']
          ,function(x)as.integer(!is.na(x))
        )
      ,by='subject_id'
    ) %>%
    
    # Join with nationwide medical history with censoring probability up to 2 y
    right_join(
      mh_nationwide %>%
        filter(subject_id %in% set$intv$subject_id) %>%
        group_by(subject_id) %>%
        mutate(cens_p=(365*2-max(day_to_event))/(365*2)) %>%
        ungroup()
      ,by=c('subject_id','day_to_event')
    ) %>%
    
    # Convert outcome as 0 and 1, then re-arrange columns
    mutate(outcome=as.integer(outcome=='event')) %>%
    select(subject_id,day_to_event,outcome,cens_p,everything()) %>%
    
    # Make  a sequence column and select codes
    # that were used for making causal predictors
    mutate(seq=seq(nrow(.))) %>%
    select_at(unique(c(
      'subject_id','day_to_event','outcome','cens_p','age_m','age_s','seq'
      ,colnames(.) %>%
        .[str_detect(str_to_upper(.)
                     ,paste(dag$measure_nodes$name,collapse='|')
          )]
    ))) %>%
    
    # Stack predictor columns
    pivot_longer(
      colnames(.) %>%
        .[!.%in%c('subject_id'
                  ,'day_to_event'
                  ,'outcome'
                  ,'cens_p'
                  ,'age_m'
                  ,'age_s'
                  ,'age'
                  ,'seq')]
      ,names_to='predictor',values_to='value'
    ) %>%
    as.data.frame() %>%
    
    # Make causal predictors from the codes,
    # but set the values as 0 (no) or 1 (yes), instead of a frequency
    pblapply(X=seq(nrow(dag$measure_nodes))
             ,Y=dag$measure_nodes$name
             ,Z=dag$measure_nodes$label
             ,K=.
             ,function(X,Y,Z,K){
      if(!(Z[X]=='Y01*'|Y[X]=='AGE')){
        filter(K,str_detect(str_to_upper(predictor),Y[X])) %>%
          group_by(
            subject_id
            ,day_to_event
            ,outcome
            ,cens_p
            ,age_m
            ,age_s
            ,age
            ,seq
          ) %>%
          summarize(
            predictor=str_remove_all(Z[X],'\\*')
            ,value=as.integer(sum(value)>0)
          ) %>%
          ungroup()
      }
    }) %>%
    do.call(rbind,.) %>%
    
    # If yes, turn values as probabilities up to 2 y
    mutate(value=value*(365*2-day_to_event)/(365*2)) %>%
    
    # Spread the predictors as individual columns
    spread(predictor,value) %>%
    select(-seq) %>%
    
    # Finishing touch
    rename(A20=age) %>%
    select_at(colnames(.) %>% .[order(.)]) %>%
    select(subject_id,day_to_event,outcome,cens_p,age_m,age_s,everything())
  
  cat('End:',as.character(now()))
  saveRDS(set$inference,'data/inference.rds')
}else{
  cat(readRDS('data/log.rds')[['inference']])
  set$inference=readRDS('data/inference.rds')
}
```

Then, we looked at each causal diagram and made a model consisting a causal
factor of interest and the confounding factors, as described in the previous
example. The dataset for causal inference was used but we convert all non-zero
probability of medical histories into value of 1. We also used G-estimation
function, as previously described.

```{r Prepare formula + binary data + G-estimation function, include=FALSE}
# Formula
dag$formula$A02=
  outcome~
  A02+A20+A25 # backdoor: A21
dag$formula$A03=
  outcome~
  A03+A15+L37+A28+A04+A05+A10+A11+A13+A02+
  L30+A20+A24+L34+A14+L33+L35+A25+A19 # backdoor: A08, A21, A23, A22
dag$formula$A04=
  outcome~
  A04+A02+A05+A10+A11+A13+L30+A20+A25+A24+A14+
  L34+L33+L35+L37+L38+A15+A28+A19 # backdoor: A21, A23, A26, A22, A08
dag$formula$A05=
  outcome~
  A05+A20+A10+A24+L30+A14+L34+A19+A15+A28 # backdoor: A21, A23, A26
dag$formula$A06=
  outcome~
  A06+A19+A09+A20+A25+L30+A05+A10+A24+
  L34+A14+A19+A15+A28 # backdoor: A08, A21, A27, A22, A23, A26
dag$formula$A09=
  outcome~
  A09+A19+A20 # backdoor: A08, A21, A22, A23
dag$formula$A10=
  outcome~
  A10+L34+A14+A19+A15+A28 # backdoor: A21
dag$formula$A11=
  outcome~
  A11+A15+A20+L33+
  L34+L35+L37+A28 # backdoor: A22, A23, A08
dag$formula$A12=
  outcome~
  A12+A02+A25+
  L34+L35+A20 # backdoor: A21
dag$formula$A13=
  outcome~
  A13+A15+A28+
  L35+L37+L38 # backdoor:
dag$formula$A14=
  outcome~
  A14+A15+A28+A19+
  L37 # backdoor: A21
dag$formula$A15=
  outcome~
  A15+A28+
  L37 # backdoor: 
dag$formula$A19=
  outcome~
  A19 # backdoor: 
dag$formula$A20=
  outcome~
  A20 # backdoor: 
dag$formula$A24=
  outcome~
  A24 # backdoor: 
dag$formula$A25=
  outcome~
  A25 # backdoor:
dag$formula$A28=
  outcome~
  A28 # backdoor:

# Binary data
bn_exp_data=
  set$inference %>%
  lapply(X=1:2,Y=.,function(X,Y){
    if(X==1){
      select(Y,outcome,A20)
    }else{
      Y %>%
        select(
          -subject_id
          ,-day_to_event
          ,-outcome
          ,-cens_p
          ,-age_m
          ,-age_s
          ,-A20
        ) %>%
        mutate_all(function(x)as.integer(x>0))
    }
  }) %>%
  do.call(cbind,.) %>%
  select_at(c('outcome',colnames(.) %>% .[.!='outcome'] %>% .[order(.)])) %>%
  mutate(
    A20=as.integer(!between(
      A20
      ,(18-set$inference$age_m[1])/set$inference$age_s[1]
      ,(35-set$inference$age_m[1])/set$inference$age_s[1]
    ))
  )

# G-estimation function
g_estimation=function(formula
                      ,data
                      ,R=5
                      ,par=0
                      ,lower=-10
                      ,upper=10
                      ,parallel.run=T
                      ,ncpus=detectCores()-1
                      ,verbose=T){
  AbsCoefHpsi=function(formula,data){
    Hpsi=function(outcome,exposure){function(psi){outcome-psi*exposure}}
    function(psi){
      outcome=data[[as.character(formula)[2]]]
      exposure=data[[str_split(as.character(formula)[3],' \\+ ')[[1]][1]]]
      cov_names=c(str_split_fixed(as.character(formula)[3],' \\+ ',2))
      cov_names[2]=ifelse(cov_names[2]=='',1,cov_names[2])
      exposure.formula=eval(parse(text=paste0(cov_names,collapse=' ~ ')))
      model=suppressWarnings(
        glm(exposure.formula,data,family=binomial(link='logit'))
      )
      data$..Hpsi..=(Hpsi(outcome,exposure))(psi)
      model2=suppressWarnings(update(model,formula=~.+..Hpsi..))
      tail(coef(model2),1) %>% abs
    }
  }
  bs_func=function(X,formula,data,AbsCoefHpsi){
    data=data %>% .[sample(seq(nrow(.)),nrow(.),T),]
    AbsCoefHpsiFun=AbsCoefHpsi(formula,data)
    results=suppressWarnings(
      optim(par=par,AbsCoefHpsiFun,lower=lower,upper=upper)
    )
    c(results$par
      ,results$value
      ,results$counts[1]
      ,results$counts[2]
      ,results$convergence)
  }
  if(parallel.run){
    cl=makeCluster(ncpus)
    clusterEvalQ(cl,{
      library('parallel')
      library('tidyverse')
    })
    b=pbsapply(
      X=seq(R)
      ,formula=formula
      ,data=data
      ,AbsCoefHpsi=AbsCoefHpsi
      ,cl=cl
      ,bs_func
    )
    stopCluster(cl)
  }else{
    if(verbose){
      b=pbsapply(
        X=seq(R)
        ,formula=formula
        ,data=data
        ,AbsCoefHpsi=AbsCoefHpsi
        ,bs_func
      )
    }else{
      b=sapply(
        X=seq(R)
        ,formula=formula
        ,data=data
        ,AbsCoefHpsi=AbsCoefHpsi
        ,bs_func
      )
    }
  }
  t(b) %>%
    `dimnames<-`(
      list(NULL,c('par','psi','counts.function','counts.gradient','converge'))
    ) %>%
    as.data.frame() %>%
    lapply(X=1,Y=.,function(X,Y){
      rbind(
          Y %>%
            summarize_all(mean) %>%
            mutate(metric='G_estimate')
          ,Y %>%
            summarize_all(function(x){
              mean(x)-qnorm(0.975)*sd(x)/sqrt(length(x))
            }) %>%
            mutate(metric='G_lb')
          ,Y %>%
            summarize_all(function(x){
              mean(x)+qnorm(0.975)*sd(x)/sqrt(length(x))
            }) %>%
            mutate(metric='G_ub')
        ) %>%
        column_to_rownames(var='metric') %>%
        t() %>%
        as.data.frame() %>%
        rownames_to_column(var='metric')
    }) %>%
    .[[1]] %>%
    mutate(OR=exp(G_estimate),OR_lb=exp(G_lb),OR_ub=exp(G_ub)) %>%
    mutate(Pr=OR/(1+OR),Pr_lb=OR_lb/(1+OR_lb),Pr_ub=OR_ub/(1+OR_ub))
}
```

First, we used logistic regression to conduct causal inference using the same
formula. Then, we used G-estimation. If value of 1 is not covered within the 95%
confidence interval odds ratio (OR), then the causal factor of interest is
confirmed having causal relationship, assuming the causal model (diagram and
formula) is correct. The results of both methods are shown (Table 5). Finally,
we used results from G-estimation to determine causal factors. A previous study
showed G-estimation was robust to reconstruct a model to make the simulated data
for >95% of times by Monte Carlo simulation, although the causal diagram or
formula used for G-estimation was incorrectly defined.

```{r Conduct logistic regression for causal inference, include=FALSE}
dag$glm=
  dag$formula %>%
  lapply(glm,binomial(link='logit'),bn_exp_data)

dag$coef=
  dag$glm %>%
  lapply(tidy) %>%
  lapply(
    mutate
    ,OR=exp(estimate)
    ,OR_lb=exp(estimate-qnorm(0.975)*std.error)
    ,OR_ub=exp(estimate+qnorm(0.975)*std.error)
    ,Pr=OR/(1+OR)
    ,Pr_lb=OR_lb/(1+OR_lb)
    ,Pr_ub=OR_ub/(1+OR_ub)
  )
```

```{r Conduct G-estimation for causal inference ..., include=FALSE}
if(run_heavy_computation){
  cat('Conduct G-estimation for causal inference',append=T)
  cat('based on the theoretical causal diagram\n')
  cat('Started:',as.character(now()),'\n')
  
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    dag$gest=
      dag$formula %>%
      pblapply(g_estimation,data=bn_exp_data,R=30,ncpus=detectCores()-1)
  
  cat('End:',as.character(now()))
  saveRDS(dag$gest,'data/g_estimation.rds')
}else{
  cat(readRDS('data/log.rds')[['g_estimation']])
  dag$gest=readRDS('data/g_estimation.rds')
}
```

```{r echo=FALSE}
dag$sig=
  lapply(X=1:2,Y=dag$coef,Z=dag$gest,function(X,Y,Z){
    if(X==1){
      sapply(Y,function(x){
        ifelse(
          between(1,round(x$OR_lb[2],4),round(x$OR_ub[2],4))|is.na(x$OR[2])
          ,0,1
        )
      })
    }else{
      sapply(Z,function(x){
        ifelse(
          between(1,round(x$OR_lb[2],4),round(x$OR_ub[2],4))|is.na(x$OR[2])
          ,0,1
        )
      })
    }
  }) %>%
  do.call(rbind,.) %>%
  `rownames<-`(c('glm','gest'))

dag$sig %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var='variable') %>%
  mutate_at(2:3,function(x)ifelse(x==1,'yes','no')) %>%
  left_join(rename(dag$baseline_nodes,variable=label),by='variable') %>%
  mutate_at(c('glm','gest'),str_to_sentence) %>%
  select(variable,name,everything()) %>%
  setNames(
    c('Causal code'
      ,'Causal factor of interest'
      ,'Logistic Regression'
      ,'G-estimation'
  )) %>%
  kable(format='latex',caption='Causal inference') %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

## Any insights gained

If we look at the causal diagram of the confirmed causal factors (Figure 3),
we could find several microbial-related factors. Other medical factors, such as
assisted reproduction and uterine anomaly, were considerably differential to
the microbial-related factors, and theoretically the most antecedents over 
other factors.

```{r figure-3, echo=FALSE, fig.cap='Final causal diagram', fig.height=10.5, fig.width=10.5}
par(bg='gray80')
dag$baseline_edges %>%
  filter(
    from %in% colnames(dag$sig)[dag$sig[2,]==1] &
    to %in% c('Y01',colnames(dag$sig)[dag$sig[2,]==1])
  ) %>%
  left_join(
    dag$baseline_nodes %>%
      rename(from=label) %>%
      mutate(label=paste(from,name)) %>%
      select(from,label)
    ,by='from'
  ) %>%
  select(-from) %>%
  rename(from=label) %>%
  left_join(
    dag$baseline_nodes %>%
      rename(to=label) %>%
      mutate(label=paste(to,name)) %>%
      select(to,label)
    ,by='to'
  ) %>%
  select(-to) %>%
  rename(to=label) %>%
  plot_dag(0,'Y01 Outcome','all',T)
```

However, we realized no data being provided for other causal factor candidates
(Table 6). These factors were also confounding factors in causal diagrams of
other causal factors of which data were available. Except conization, none of
the confounding factors is clinically relevant as causal factors. Conization is
not considered as a common intervention. In addition, the prevalence of the
outcome is conceivably very high in relative to conization prevalence.

```{r echo=FALSE}
dag$baseline_nodes %>%
  filter(label %in% c('A08','A21','A23','A22','A26','A27')) %>%
  setNames(c('Unblock causal factor candidates','Causal code')) %>%
  kable(format='latex',caption='Unblocked confounding factors') %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

Considering these insights, we decided to include the causal factors into
predictor candidates. Of course, we still used all predictors, including any
medical history, without considering whether these are confounding, since
predictive modeling have a very different principle to etiologic modeling. It
is also reasonable if a prediction model using only causal factors may have
worse predictive performance compared to another model using any predictors. Our
approach by using causal inference is to get more insights, not only for
improving predictive performance or model generalizability, but also developing
a prevention strategy in future studies.

## Modeling approach

We applied five approaches to develop the predictions models. One of them using
only causal factors as predictors/features, while the remaining used principal
components (PCs) as features. Then, we applied regression model. The selected
PCs were used for last three models using different algorithms. In the end,
all models would be calibrated. These are five prediction modeling approaches:

1. Ridge regression using only causal predictors (Causal LR)
2. Elastic net regression using PCs (PC-LR)
3. Random forest using selected PCs (PC-RF)
4. Linear discriminant analysis using selected PCs (PC-LDA)
5. Gradient boosting machine using selected PCs (PC-GBM)

Before developing any models, we constructed a training set using any
predictors, including medical histories and causal predictors in isolated
provider. Convert each of the predictors into proportions of days up to 2 years
before the event, of which a predictor (a diagnosis/procedure code) being
encountered. We subset only internal validation set and add a predictor of
censoring probability, as previously described. In the end, we balanced the
outcome by naive random oversampling to combat class imbalance problem.

```{r Construct a class-balanced training set by provider ..., include=FALSE}
if(run_heavy_computation){
  cat('Construct a class-balanced training set by provider',append=T)
  cat('with all and represented features\n')
  cat('Started:',as.character(now()),'\n')
  
  pb=startpb(0,5)
  on.exit(closepb(pb))
  setpb(pb,0)
  
  # Use provider medical history
  setpb(pb,1)
  temp=
    mh_provider %>%
    select(-subject_id,-day_to_event,-est_strata_id) %>%
    t() %>%
    t()
  
  temp=
    (temp>0) %>%
    `storage.mode<-`('integer')
  
  # Make the confirmed causal predictors defined by provider medical histories
  setpb(pb,2)
  temp2=
    lapply(
      X=seq(sum(
          str_remove_all(dag$measure_nodes$label,'\\*') %in%
            colnames(dag$sig)[dag$sig[2,]==1]
        ))
      ,Y=
        dag$measure_nodes %>%
        filter(
          str_remove_all(label,'\\*') %in% colnames(dag$sig)[dag$sig[2,]==1]
        ) %>%
        pull(name)
      ,Z=
        dag$measure_nodes %>%
        filter(
          str_remove_all(label,'\\*') %in% colnames(dag$sig)[dag$sig[2,]==1]
        ) %>%
        pull(label)
      ,K=temp
      ,function(X,Y,Z,K){
        K=K %>%
          .[,str_detect(str_to_upper(colnames(.)),Y[X]),drop=F] %>%
          rowSums() %>%
          matrix() %>%
          `dimnames<-`(
            list(rownames(K),paste0('causal_',str_remove_all(Z[X],'\\*')))
          )
        
        K=(K>0) %>%
          `storage.mode<-`('integer')
        
    }) %>%
    do.call(cbind,.)
  
  # Combine medical history and the causal predictors,
  # then convert frequency into probability or proportion of day being
  # encountered up to 2 years before the event
  setpb(pb,3)
  temp=
    cbind(temp2,temp) %>%
    sweep(1,((365*2-mh_provider$day_to_event)/(365*2)),'*')
  rm(temp2)
  
  
  # Join the results with other attributes for data partition
  temp=
    mh_provider %>%
    select(subject_id,day_to_event,est_strata_id) %>%
    cbind(as.data.frame(temp))
  
  # Get non-medical history predictors
  setpb(pb,4)
  set$training=
    
    ## Numerical predictors (age), standardize, and normalize intor 0 to 1
    readRDS('data/target_population.rds') %>%
    select(subject_id,day_to_event,outcome,age) %>%
    .[!duplicated(.),] %>%
    filter(subject_id %in% set$intv$subject_id) %>%
    mutate(
      age_m=mean(age)
      ,age_s=sd(age)
      ,age=(qnorm(0.975)+(age-age_m)/age_s)/(2*qnorm(0.975))
    ) %>%
    
    ## Categorical predictors and getting these cleaned
    left_join(
      readRDS('data/target_population.rds') %>%
        select(
          subject_id
          ,marital_status
          ,insurance_class
          ,occupation_segment
          ,healthcare_class
        ) %>%
        .[!duplicated(.),] %>%
        group_by(subject_id) %>%
        summarize_all(function(x){
          x[!duplicated(x)] %>%
            .[which.max(
              sapply(x[!duplicated(x)],function(x2)sum(x2==x,na.rm=T))
            )]
        }) %>%
        lapply(X=seq(ncol(.)),Y=.,function(X,Y){
          if(colnames(Y)[X] %in% c('marital_status'
                                   ,'insurance_class'
                                   ,'occupation_segment'
                                   ,'healthcare_class')){
            Y[,c('subject_id',colnames(Y)[X]),drop=F] %>%
              rename_at(colnames(Y)[X],function(x)'key') %>%
              mutate(
                key=str_replace_all(key,'[:punct:]|[:space:]','_'),value=1
              ) %>%
              mutate_at('key',function(x)paste0(colnames(Y)[X],'.',x)) %>%
              spread(key,value) %>%
              select(-subject_id)
          }else{
            Y[,colnames(Y)[X],drop=F]
          }
        }) %>%
        do.call(cbind,.) %>%
        mutate_at(
          colnames(.) %>% .[.!='subject_id']
          ,function(x)as.integer(!is.na(x))
        )
      ,by='subject_id'
    ) %>%
    
    ## Subset internal validation set and add censoring probability
    right_join(
      temp %>%
        filter(subject_id %in% set$intv$subject_id) %>%
        group_by(subject_id) %>%
        mutate(cens_p=(365*2-max(day_to_event))/(365*2)) %>%
        ungroup()
      ,by=c('subject_id','day_to_event')
    ) %>%
    
    # Convert outcome as 0 and 1
    mutate(outcome=as.integer(outcome=='event')) %>%
    
    # Pre-finishing touch
    mutate(causal_A20=age) %>%
    select(subject_id,day_to_event,est_strata_id,outcome,cens_p,everything())
  rm(temp)
  
  
  # Upsample minority to balance the outcome
  setpb(pb,5)
  suppressWarnings(set.seed(33,sample.kind=sample.kind))
  set$training=
    set$training %>%
    upSample(factor(.$outcome)) %>%
    mutate(
      Class=
        ifelse(Class==0,'non_event','event') %>%
        factor(c('event','non_event'))
    ) %>%
    select(
      subject_id
      ,day_to_event
      ,outcome
      ,Class
      ,age_m
      ,age_s
      ,cens_p
      ,everything()
    )
  
  cat('\nEnd:',as.character(now()))
  rm(pb)
  saveRDS(set$training,'data/training.rds')
}else{
  cat(readRDS('data/log.rds')[['training']])
}
```

```{r Create an empty list to save model, include=FALSE}
if(run_heavy_computation){
  model=list()
}
```

### Model development

#### Ridge regression using only causal predictors (Causal LR)

We used ridge regression for causal LR because we need to retain all of the
causal predictors. In this regression model, L2-norm regularization is applied
as the shrinkage method. This means alpha being set to 0 while lambda was tuned
using grid search from 10e-9 to 1 split into 10 numbers evenly. We applied
6-fold cross validation for determining the best tuning parameter (i.e. lambda)
based on the area under receiver operating characteristics (AUROC). Using the
best lambda, we trained the model and used 30-times bootstrapping to get
validation AUROC interval estimates.

```{r Conduct ridge regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  cat('Conduct ridge regression by parallel computing\n')
  cat('Started:',as.character(now()),'\n')
  pb=startpb(0,3)
  on.exit(closepb(pb))
  setpb(pb,0)
  cl=makePSOCKcluster(detectCores()-1)
  registerDoParallel(cl)
  clusterEvalQ(cl,{
    library('parallel')
    library('doParallel')
    library('tidyverse')
    library('pbapply')
    library('caret')
    library('glmnet')
  })
    
    setpb(pb,1)
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    model$ridge=
      train(
        Class~.
        ,data=
          set$training %>%
          .[,c('Class','cens_p',colnames(.) %>% .[str_detect(.,'causal')])]
        ,method='glmnet'
        ,metric='ROC'
        ,trControl=
          trainControl(
            'cv'
            ,number=6
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneGrid=data.frame(alpha=0,lambda=10^seq(-9,0,len=10))
      )
    
    setpb(pb,2)
    model$ridge=
      train(
        Class~.
        ,data=
          set$training %>%
          .[,c('Class','cens_p',colnames(.) %>% .[str_detect(.,'causal')])]
        ,method='glmnet'
        ,metric='ROC'
        ,trControl=
          trainControl(
            method='boot'
            ,number=30
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneGrid=data.frame(alpha=0,lambda=model$ridge$bestTune$lambda)
      )
    
  stopCluster(cl)
  registerDoSEQ()
  rm(cl)
  gc()
  setpb(pb,3)
  cat('\nEnd:',as.character(now()))
  rm(pb)
  saveRDS(model$ridge,paste0('data/ridge.rds'))
}else{
  cat(readRDS('data/log.rds')[['ridge']])
}
```

#### Elastic net regression using PCs (PC-LR)

Before developing the remaining prediction models, we filter medical history
predictors, including causal ones, that have non-zero variance to avoid perfect
separation problem. This means one of two outcome having only one of two
categories in a predictor. Although this may be true relationship between the
predictor and the outcome, this situation may also happen by chance, or sampling
error. Therefore, we removed this kind of predictor. We determine the variance by
only using training set (internal validation).

```{r Only use predictor with no perfect separation, include=FALSE}
if(run_heavy_computation){
  set$no_per_sep=
    set$training %>%
    select(-subject_id,-day_to_event,-outcome,-age_m,-age_s,-est_strata_id) %>%
    group_by(Class) %>%
    summarize_all(sd) %>%
    gather(predictor,value,-Class) %>%
    spread(Class,value) %>%
    filter(!(event==0 | non_event==0))
}
```

We computed PCs using only training set by 6-fold cross validation. We
standardized all predictors using average and standard deviation of training
partition for each fold. This means we had 6 versions of PCs.

```{r Conduct NPS cross-validated PCA, include=FALSE}
if(run_heavy_computation){
  cat('Conduct NPS cross-validated PCA\n')
  cat('Started:',as.character(now()),'\n')
    
    model$pca_nps=
      set$training %>%
      .[,c('subject_id',set$no_per_sep$predictor)] %>%
      pblapply(X=seq(length(set$cv_idx)),Y=set$cv_idx,Z=.,function(X,Y,Z){
        K=Z %>%
          filter(!subject_id %in% Y[[X]]) %>%
          select(-subject_id)
        
        K=K[,K %>%
              summarise_all(sd) %>%
              gather() %>%
              filter(value!=0) %>%
              pull(key)
            ]
        
        L=summarize_all(K,mean) %>%
          gather()
        L=setNames(L$value,L$key)
        M=summarize_all(K,sd) %>%
          gather()
        M=setNames(M$value,M$key)
        N=as.matrix(K) %>%
          sweep(2,L,'-') %>%
          sweep(2,M,'/') %>%
          prcomp()
        
        list(mean=L,sd=M,prcomp=N)
      })
    
  gc()
  cat('End:',as.character(now()))
  saveRDS(model$pca_nps,'data/pca_nps.rds')
}else{
  cat(readRDS('data/log.rds')[['pca_nps']])
}
```

Then, we used top PCs that contributed cumulative percentage of variance
explained for 50% at minimum (filter method). We simply determined this
proportion to reduce the number of predictor candidates for further selection
using PC-LR model (wrapper method).

```{r Save min. NPS PCs to get 50% cum. pct. of var. explained, include=FALSE}
if(run_heavy_computation){
  pca_nps_pve50=
    model$pca_nps %>%
    sapply(X=seq(length(.)),Y=.,function(X,Y){
      cum_pve=cumsum(Y[[X]]$prcomp$sdev^2/sum(Y[[X]]$prcomp$sdev^2))
      print(plot(cum_pve))
      which(cum_pve>=0.5)[1]
    }) %>%
    max()
}
```

To transform original predictors into PCs using 6 versions, we used estimates
of mean, standard deviation, and the weights for each PC to transform all
predictors into each PC. The estimates were computed by average. Intuition
behind this calculation is that we tried to estimate PCs in population
by cross validation. This procedure is similar to a linear combination of
all predictors into a PC, but the weights for each PC are determined using PC
analysis (PCA) pipeline. In PCA, a later PC is perpendicular to earlier one in
low dimension. This is repeated until 100% variance explained and the number of
maximum PCs are the same with the number of predictors that are projected.
Therefore, we projected predictors into lower dimension but retaining at least
50% variance given by all predictors.

```{r A function to represent predictors as the CV PCA, include=FALSE}
pc_converter=function(data,pca,npc=NULL){
  
  pb=startpb(0,7)
  on.exit(closepb(pb))
  setpb(pb,0)
  
  # Get rotation of the PC matrix (PC weights as columns)
  if(is.null(npc)) npc=seq(ncol(pca[[1]]$prcomp$rotation))
  
  # Subset the top PCs and stack all versions of PCs
  setpb(pb,1)
  rotated_pc=
    pca %>%
    lapply(X=seq(length(.)),Y=.,function(X,Y){
      as.data.frame(Y[[X]]$prcomp$rotation[,npc]) %>%
        rownames_to_column(var='predictor')
    }) %>%
    do.call(rbind,.)
  
  # Group by predictor and average the PC weights over all versions
  setpb(pb,2)
  rotated_pc=
    rotated_pc %>%
    group_by(predictor) %>%
    summarize_all(function(x)mean(x,na.rm=T)) %>%
    ungroup() %>%
    column_to_rownames(var='predictor')
  
  # Get mean and standard deviation for each version
  setpb(pb,3)
  scaler=
    pca %>%
    lapply(X=seq(length(.)),Y=.,function(X,Y){
      data.frame(predictor=names(Y[[X]]$mean),mean=Y[[X]]$mean,sd=Y[[X]]$sd) %>%
        `rownames<-`(NULL)
    }) %>%
    do.call(rbind,.)
  
  # Average the mean and standard deviation of all versions
  setpb(pb,4)
  scaler=
    scaler %>%
    group_by(predictor) %>%
    summarize_all(function(x)mean(x,na.rm=T)) %>%
    ungroup() %>%
    column_to_rownames(var='predictor')
  
  # Standardized predictors using the estimate of mean and standard deviation
  setpb(pb,5)
  scaled_data=
    data %>%
    .[,rownames(rotated_pc)] %>%
    sweep(2,scaler$mean,'-') %>%
    sweep(2,scaler$sd,'/')
  
  # Compute dot product of standardized predictors with the averaged rotated PCs
  setpb(pb,6)
  pc_data=as.matrix(scaled_data) %*% as.matrix(rotated_pc)
  
  setpb(pb,7)
  as.data.frame(pc_data)
  
}
```

```{r Represented NPS predictors as the cross-validated PCA, include=FALSE}
if(run_heavy_computation){
  cat('Represent NPS predictors as the cross-validated PCA\n')
  cat('Started:',as.character(now()),'\n')
    
    set$training_pc_nps=pc_converter(set$training,model$pca_nps,npc=1:pca_nps_pve50)
    
  gc()
  cat('End:',as.character(now()))
  saveRDS(set$training_pc_nps,'data/training_pc_nps.rds')
}else{
  cat(readRDS('data/log.rds')[['training_pc_nps']])
}
```

We used only PCs as features for PC-LR. We also used this model for further
selection to determine feature candidates in the next models that uses other
machine learning algorithms. Non-LR machine learning algorithms were shown
data hungry. LR only needs at least 20 events per variable (EPV; predictor
candidate), while other machine learning algorithms need up to 50 to >200 EPV.
To approach the requirement, we applied this wrapper method to determine
the predictor candidates for the non-LR machine learning models.

In PC-LR, we applied elastic net regression. Random search of 10 combinations
of alpha and lambda was conducted. We used this shrinkage method to select
important PCs by L1-norm regularization while minimizing feature exclusion
by L2-norm regularization. We also included censoring probability as additional
feature beyond the PCs to include information of how much data recorded for
a patient. As causal LR, we applied the same validation techniques for
determining either the best tuning parameter or final predictive performance
using the best tuning parameter.

```{r Conduct NPS-PC elastic net regression by parallel computing, include=FALSE}
if(run_heavy_computation){
  cat('Conduct NPS-PC elastic net regression by parallel computing\n')
  cat('Started:',as.character(now()),'\n')
  pb=startpb(0,3)
  on.exit(closepb(pb))
  setpb(pb,0)
  cl=makePSOCKcluster(floor(detectCores()/2))
  registerDoParallel(cl)
  clusterEvalQ(cl,{
    library('parallel')
    library('doParallel')
    library('tidyverse')
    library('pbapply')
    library('caret')
    library('glmnet')
  })
    
    setpb(pb,1)
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    model$pc_nps_elnet=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_pc_nps)
        ,method='glmnet'
        ,metric='ROC'
        ,trControl=
          trainControl(
            'cv'
            ,number=6
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneLength=10
      )
    
    setpb(pb,2)
    model$pc_nps_elnet=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_pc_nps)
        ,method='glmnet'
        ,metric='ROC'
        ,trControl=
          trainControl(
            method='boot'
            ,number=30
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneGrid=
          data.frame(
            alpha=model$pc_nps_elnet$bestTune$alpha
            ,lambda=model$pc_nps_elnet$bestTune$lambda
          )
      )
    
  stopCluster(cl)
  registerDoSEQ()
  rm(cl)
  gc()
  setpb(pb,3)
  cat('\nEnd:',as.character(now()))
  rm(pb)
  saveRDS(model$pc_nps_elnet,'data/pc_nps_elnet.rds')
}else{
  cat(readRDS('data/log.rds')[['pc_nps_elnet']])
}
```

Since PC-LR applied L1-norm regularization, some features would be excluded;
thus, we could select the features furthermore for the next prediction models.
However, the exclusion were based on absolute weights arranged from the highest
to the lowest, not including censoring probability. We chose top n PCs that
fulfill 200 EPV. For the next models, we only used the selected PCs.

```{r The selected NPS-PC by the elastic net regression, include=FALSE}
if(run_heavy_computation){
  set$selected_pc=
    coef(model$pc_nps_elnet$finalModel,model$pc_nps_elnet$bestTune$lambda) %>%
    as.matrix() %>%
    as.data.frame() %>%
    rownames_to_column(var='predictor') %>%
    mutate(predictor=str_remove_all(predictor,'\\`|\\(|\\)')) %>%
    setNames(c('predictor','estimate')) %>%
    filter(estimate!=0) %>%
    arrange(desc(abs(estimate))) %>%
    filter(!predictor %in% c('Intercept','cens_p')) %>%
    slice(1:20) %>%
    mutate(idx=str_remove_all(predictor,'PC') %>% as.integer())
  
  saveRDS(set$selected_pc,'data/selected_pc.rds')
}else{
  set$selected_pc=readRDS('data/selected_pc.rds')
}
```

```{r Represented NPS predictors as the selected and CV PCA, include=FALSE}
if(run_heavy_computation){
  cat('Represent NPS predictors as the selected and cross-validated PCA\n')
  cat('Started:',as.character(now()),'\n')
    
    set$training_spc_nps=pc_converter(set$training,model$pca_nps,npc=sort(set$selected_pc$idx))
    
  gc()
  cat('End:',as.character(now()))
  saveRDS(set$training_spc_nps,'data/training_spc_nps.rds')
}else{
  cat(readRDS('data/log.rds')[['training_spc_nps']])
}
```

#### Random forest using selected PCs (PC-RF)

We used random forest because it is one of two competition-winning models in
machine learning. This model takes some features and some samples multiple times
to construct multiple classification trees in parallel. Prediction is conducted
by ensemble of the tree predictions. This deals with non-linear problem. We
applied the same strategy with PC-LR for tuning parameter and final training,
including the validation techniques.

```{r Conduct selected NPS-PC random forest by parallel computing, include=FALSE}
if(run_heavy_computation){
  cat('Conduct selected NPS-PC random forest by parallel computing\n')
  cat('Started:',as.character(now()),'\n')
  pb=startpb(0,3)
  on.exit(closepb(pb))
  setpb(pb,0)
  cl=makePSOCKcluster(floor(detectCores()/2))
  registerDoParallel(cl)
  clusterEvalQ(cl,{
    library('parallel')
    library('doParallel')
    library('tidyverse')
    library('pbapply')
    library('caret')
    library('Rborist')
  })
    
    setpb(pb,1)
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    model$spc_nps_rf=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_spc_nps)
        ,method='Rborist'
        ,metric='ROC'
        ,trControl=
          trainControl(
            'cv'
            ,number=6
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneLength=10
      )
    
    setpb(pb,2)
    model$spc_nps_rf=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_spc_nps)
        ,method='Rborist'
        ,metric='ROC'
        ,trControl=
          trainControl(
            method='boot'
            ,number=30
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneGrid=
          data.frame(
            predFixed=model$spc_nps_rf$bestTune$predFixed
            ,minNode=model$spc_nps_rf$bestTune$minNode
          )
      )
    
  stopCluster(cl)
  registerDoSEQ()
  rm(cl)
  gc()
  setpb(pb,3)
  cat('\nEnd:',as.character(now()))
  rm(pb)
  saveRDS(model$spc_nps_rf,'data/spc_nps_rf.rds')
}else{
  cat(readRDS('data/log.rds')[['spc_nps_rf']])
}
```

#### Linear discriminant analysis using selected PCs (PC-LDA)

We also used LDA in assumption of linearity between PCs and the outcome. Using
continous independent variables from PCs, this model explicitly attempts to
model the outcome difference. We also used the selected PCs based on PC-LR
to get features that follows more strict assumptions of LDA. Therefore, we
expected a similar model to LR, but approaching the outcome slightly different
to the PC-LR. We applied the same strategy with PC-LR for tuning parameter and
final training, including the validation techniques.

```{r Conduct selected NPS-PC LDA by parallel computing, include=FALSE}
if(run_heavy_computation){
  cat('Conduct selected NPS-PC LDA by parallel computing\n')
  cat('Started:',as.character(now()),'\n')
  pb=startpb(0,3)
  on.exit(closepb(pb))
  setpb(pb,0)
  cl=makePSOCKcluster(floor(detectCores()/2))
  registerDoParallel(cl)
  clusterEvalQ(cl,{
    library('parallel')
    library('doParallel')
    library('tidyverse')
    library('pbapply')
    library('caret')
    library('lda')
  })
    
    setpb(pb,1)
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    model$spc_nps_lda=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_spc_nps)
        ,method='lda'
        ,metric='ROC'
        ,trControl=trainControl(
          'cv'
          ,number=6
          ,summaryFunction=twoClassSummary
          ,classProbs=T
          ,savePredictions=T
          ,allowParallel=T
        )
        ,tuneLength=10
      )
    
    setpb(pb,2)
    model$spc_nps_lda=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_spc_nps)
        ,method='lda'
        ,metric='ROC'
        ,trControl=
          trainControl(
            method='boot'
            ,number=30
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneGrid=data.frame(parameter=model$spc_nps_lda$bestTune$parameter)
      )
    
  stopCluster(cl)
  registerDoSEQ()
  rm(cl)
  gc()
  setpb(pb,3)
  cat('\nEnd:',as.character(now()))
  rm(pb)
  saveRDS(model$spc_nps_lda,'data/spc_nps_lda.rds')
}else{
  cat(readRDS('data/log.rds')[['spc_nps_lda']])
}
```

#### Gradient boosting machine using selected PCs (PC-GBM)

The last model used another competition-winning model. Unlike random forest,
this model constructs trees sequentially. Later tree predicts error of the
earlier one. By using this model, we applied two linear models, and two
non-linear models beyond the causal LR. We applied the same strategy with PC-LR
for tuning parameter and final training, including the validation techniques.

```{r Conduct selected NPS-PC GBM by parallel computing, include=FALSE}
if(run_heavy_computation){
  cat('Conduct selected NPS-PC GBM by parallel computing\n')
  cat('Started:',as.character(now()),'\n')
  pb=startpb(0,3)
  on.exit(closepb(pb))
  setpb(pb,0)
  cl=makePSOCKcluster(floor(detectCores()/2))
  registerDoParallel(cl)
  clusterEvalQ(cl,{
    library('parallel')
    library('doParallel')
    library('tidyverse')
    library('pbapply')
    library('caret')
    library('gbm')
  })
    
    setpb(pb,1)
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    model$spc_nps_gbm=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_spc_nps)
        ,method='gbm'
        ,metric='ROC'
        ,trControl=
          trainControl(
            'cv'
            ,number=6
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneLength=10
      )
    
    setpb(pb,2)
    model$spc_nps_gbm=
      train(
        Class~.
        ,data=
          set$training %>%
          select(Class,cens_p) %>%
          cbind(set$training_spc_nps)
        ,method='gbm'
        ,metric='ROC'
        ,trControl=
          trainControl(
            method='boot'
            ,number=30
            ,summaryFunction=twoClassSummary
            ,classProbs=T
            ,savePredictions=T
            ,allowParallel=T
          )
        ,tuneGrid=data.frame(
          n.trees=model$spc_nps_gbm$bestTune$n.trees
          ,interaction.depth=model$spc_nps_gbm$bestTune$interaction.depth
          ,shrinkage=model$spc_nps_gbm$bestTune$shrinkage
          ,n.minobsinnode=model$spc_nps_gbm$bestTune$n.minobsinnode
        )
      )
    
  stopCluster(cl)
  registerDoSEQ()
  rm(cl)
  gc()
  setpb(pb,3)
  cat('\nEnd:',as.character(now()))
  rm(pb)
  saveRDS(model$spc_nps_gbm,'data/spc_nps_gbm.rds')
}else{
  cat(readRDS('data/log.rds')[['spc_nps_gbm']])
}
```
```{r Create empty list to save reported data, include=FALSE}
reported_data=list()
```

### Model calibration

All prediction models were calibrated by univariable logistic regression using
the predicted probabilities. Before calibration, we explored the calibration
plot and ROC curve. Calibration intercept and slope, and the AUROC, were also
computed.

```{r Create calibration plot before calibration, include=FALSE}
if(run_heavy_computation){
  model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      mutate(
          Y[[X]]$pred
          ,event=round(event,1)
          ,obs=as.integer(obs=='event')
        ) %>%
        group_by(event) %>%
        summarize(
          model=factor(X,names(Y) %>% .[!.%in%c('pca','pca_nps')])
          ,obs=mean(obs)
          ,lb=mean(obs)-qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
          ,ub=mean(obs)+qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
        )
    }) %>%
    do.call(rbind,.) %>%
    saveRDS('data/report_pre_calib.rds')
}else{
  reported_data$pre_calib=readRDS('data/report_pre_calib.rds')
}
```

```{r Compute probability distribution before calibration, include=FALSE}
if(run_heavy_computation){
  model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      mutate(
          Y[[X]]$pred
          ,event=round(event,1)
          ,obs=as.integer(obs=='event')
        ) %>%
        group_by(event) %>%
        summarize(
          model=factor(X,names(Y) %>% .[!.%in%c('pca','pca_nps')])
          ,n=n()
        )
    }) %>%
    do.call(rbind,.) %>%
    saveRDS('data/report_pre_caldist.rds')
}else{
  reported_data$pre_caldist=readRDS('data/report_pre_caldist.rds')
}
```

```{r Compute calibration intercept and slope before calibration, include=FALSE}
if(run_heavy_computation){
  model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      mutate(
          Y[[X]]$pred
          ,event=round(event,2)
          ,obs=as.integer(obs=='event')
        ) %>%
        group_by(event) %>%
        summarize(
          model=factor(X,names(Y) %>% .[!.%in%c('pca','pca_nps')])
          ,obs=mean(obs)
        )
    }) %>%
    do.call(rbind,.) %>%
    group_by(model) %>%
    do(tidy(lm(obs~event,data=.))) %>%
    mutate(term=str_remove_all(term,'\\(|\\)') %>% str_to_lower()) %>%
    pivot_wider(
      model
      ,names_from=c('term','term')
      ,values_from=c('estimate','std.error')
    ) %>%
    select(model,estimate_intercept,std.error_intercept,everything()) %>%
    mutate_at(colnames(.) %>% .[.!='model'],function(x)round(x,2)) %>%
    saveRDS('data/report_pre_calislope.rds')
}else{
  reported_data$pre_calislope=readRDS('data/report_pre_calislope.rds')
}
```

```{r Create ROC curve among models before calibration, echo=FALSE}
if(run_heavy_computation){
  model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      Y[[X]]$results %>%
        mutate(ROC_lb=ROC-qnorm(0.975)*ROCSD,ROC_ub=ROC+qnorm(0.975)*ROCSD) %>%
        mutate(model=X) %>%
        select(model,ROC,ROC_lb,ROC_ub)
    }) %>%
    do.call(rbind,.) %>%
    mutate(model=reorder(model,ROC)) %>%
    saveRDS('data/report_pre_roc.rds')
}else{
  reported_data$pre_roc=readRDS('data/report_pre_roc.rds')
}
```

```{r Compute AUC of ROC among models before calibration, include=FALSE}
if(run_heavy_computation){
  model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      Y[[X]]$results %>%
        mutate(ROC_lb=ROC-qnorm(0.975)*ROCSD,ROC_ub=ROC+qnorm(0.975)*ROCSD) %>%
        mutate(model=X) %>%
        select(model,ROC,ROC_lb,ROC_ub)
    }) %>%
    do.call(rbind,.) %>%
    saveRDS('data/report_pre_auroc.rds')
}else{
  reported_data$pre_auroc=readRDS('data/report_pre_auroc.rds')
}
```

First, we computed the predicted probability using the trained models. The
training set was used to compute the probabilities. We made a new training table
for each model.

```{r Get predicted probability of training data, include=FALSE}
if(run_heavy_computation){
  cat('Get predicted probability of training data\n')
  cat('Started:',as.character(now()),'\n')
    
    set$training_calib=
      model %>%
      pblapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
        Y[[X]] %>%
          predict(Y[[X]]$trainingData,type='prob') %>%
          cbind(select(Y[[X]]$trainingData,.outcome)) %>%
          select(event,.outcome) %>%
          setNames(c('event','obs')) %>%
          mutate(obs=as.integer(obs=='event'))
      }) %>%
      setNames(names(model) %>% .[!.%in%c('pca','pca_nps')])
    
  cat('End:',as.character(now()))
  saveRDS(set$training_calib,'data/training_calib.rds')
}else{
  cat(readRDS('data/log.rds')[['training_calib']])
}
```

Then, we trained a logistic regression for each model using the predicted
probability as single predictor. We applied 30-times bootstrapping on the
training sets. These were the aforementioned new training tables consisting
the predicted probabilities for each model.

```{r Conduct calibration using predicted probability ..., include=FALSE}
if(run_heavy_computation){
  cat('Conduct calibration using predicted probability',append=T)
  cat('and outcome by logistic regression\n')
  cat('Started:',as.character(now()),'\n')
    
    calib_model=
      model %>%
      pblapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')]
               ,Y=.
               ,Z=set$training_calib
               ,function(X,Y,Z){
        suppressWarnings(set.seed(33,sample.kind=sample.kind))
        suppressWarnings(train(
          obs~event
          ,data=
            Z[[X]] %>%
            mutate(
              obs=factor(
                ifelse(obs==1,'event','non_event')
                ,c('event','non_event')
              )
            )
          ,method='glm'
          ,metric='ROC'
          ,family=binomial(link='logit')
          ,trControl=
            trainControl(
              method='boot'
              ,number=30
              ,summaryFunction=twoClassSummary
              ,classProbs=T
              ,savePredictions=T
            )
        ))
      }) %>%
      setNames(names(model) %>% .[!.%in%c('pca','pca_nps')])
    
  cat('End:',as.character(now()))
  saveRDS(calib_model,'data/calib_model.rds')
}else{
  cat(readRDS('data/log.rds')[['calib_model']])
}
```

After all prediction models were calibrated, we compared the calibration
plot. We conducted the calibration to get a better probability distribution and
expected a clinician having better confidence when interpreting the predicted
probability. Well-calibrated models can be compared for the ROC curves
furthermore. Calibration intercept and slope, and the AUROC, were also computed
to get precise comparison.

```{r Create calibration plot after calibration, include=FALSE}
if(run_heavy_computation){
  calib_model %>%
    lapply(X=names(.),Y=.,function(X,Y){
      mutate(
          Y[[X]]$pred
          ,event=round(event,1)
          ,obs=as.integer(obs=='event')
        ) %>%
        group_by(event) %>%
        summarize(
          model=factor(X,names(Y))
          ,obs=mean(obs)
          ,lb=mean(obs)-qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
          ,ub=mean(obs)+qnorm(0.975)*sqrt(mean(obs)*(1-mean(obs))/n())
        )
    }) %>%
    do.call(rbind,.) %>%
    saveRDS('data/report_post_calib.rds')
}else{
  reported_data$post_calib=readRDS('data/report_post_calib.rds')
}
```

```{r Compute probability distribution after calibration, include=FALSE}
if(run_heavy_computation){
  calib_model %>%
    lapply(X=names(.),Y=.,function(X,Y){
      mutate(
          Y[[X]]$pred
          ,event=round(event,1)
          ,obs=as.integer(obs=='event')
        ) %>%
        group_by(event) %>%
        summarize(
          model=factor(X,names(Y))
          ,n=n()
        )
    }) %>%
    do.call(rbind,.) %>%
    saveRDS('data/report_post_caldist.rds')
}else{
  reported_data$post_caldist=readRDS('data/report_post_caldist.rds')
}
```

```{r Compute calibration intercept and slope after calibration, include=FALSE}
if(run_heavy_computation){
  calib_model %>%
    lapply(X=names(.),Y=.,function(X,Y){
      mutate(
          Y[[X]]$pred
          ,event=round(event,1)
          ,obs=as.integer(obs=='event')
        ) %>%
        group_by(event) %>%
        summarize(
          model=factor(X,names(Y))
          ,obs=mean(obs)
        )
    }) %>%
    do.call(rbind,.) %>%
    group_by(model) %>%
    do(tidy(lm(obs~event,data=.))) %>%
    mutate(term=str_remove_all(term,'\\(|\\)') %>% str_to_lower()) %>%
    pivot_wider(
      model
      ,names_from=c('term','term')
      ,values_from=c('estimate','std.error')
    ) %>%
    select(model,estimate_intercept,std.error_intercept,everything()) %>%
    mutate_at(colnames(.) %>% .[.!='model'],function(x)round(x,2)) %>%
    saveRDS('data/report_post_calislope.rds')
}else{
  reported_data$post_calislope=readRDS('data/report_post_calislope.rds')
}
```

```{r Create ROC curve among models after calibration, include=FALSE}
if(run_heavy_computation){
  calib_model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      Y[[X]]$results %>%
        mutate(ROC_lb=ROC-qnorm(0.975)*ROCSD,ROC_ub=ROC+qnorm(0.975)*ROCSD) %>%
        mutate(model=X) %>%
        select(model,ROC,ROC_lb,ROC_ub)
    }) %>%
    do.call(rbind,.) %>%
    mutate(model=reorder(model,ROC)) %>%
    saveRDS('data/report_post_roc.rds')
}else{
  reported_data$post_roc=readRDS('data/report_post_roc.rds')
}
```

```{r Compute AUC of ROC among models after calibration, include=FALSE}
if(run_heavy_computation){
  calib_model %>%
    lapply(X=names(.) %>% .[!.%in%c('pca','pca_nps')],Y=.,function(X,Y){
      Y[[X]]$results %>%
        mutate(ROC_lb=ROC-qnorm(0.975)*ROCSD,ROC_ub=ROC+qnorm(0.975)*ROCSD) %>%
        mutate(model=X) %>%
        select(model,ROC,ROC_lb,ROC_ub)
    }) %>%
    do.call(rbind,.) %>%
    saveRDS('data/report_post_auroc.rds')
}else{
  reported_data$post_auroc=readRDS('data/report_post_auroc.rds')
}
```

### Model evaluation

We chose the best model using internal validation set. Our criteria for the 
best model(s) is a well-calibrated model that significantly outperform AUROCs
of other models by 95% confidence interval. Well-calibrated model is one with 
intercept and slope interval estimates covering 0 and 1, respectively. By
visualizing calibration plot and probability distribution, well-calibrated model
should show evenly distributed probability around the reference line. Meanwhile,
a model outperforms others if the AUROC interval estimates are higher than those
of others without overlapping. More than one models may be selected.
Nevertheless, the best model based on external validation did not changed the
best model selection by internal validation.

### Model validation

For external validation, we construct testing set for each splitting method and
one combining all splits. We applied the same pipeline to construct training
set except we used the mean and standard deviation of age in training set to
standardize age in the testing sets. We also used all versions of the PC weights
inferred from the training set.

```{r Construct a class-balanced testing set by provider ..., include=FALSE}
if(run_heavy_computation){
  mh_provider=readRDS('data/mh_provider.rds')
  construct_testing_set=function(testing_idx_db){
    
    # Use provider medical history
    temp=
      mh_provider %>%
      select(-subject_id,-day_to_event,-est_strata_id) %>%
      t() %>%
      t()
    
    temp=
      (temp>0) %>%
      `storage.mode<-`('integer')
    
    # Make the confirmed causal predictors defined by provider medical histories
    temp2=
      lapply(
        X=seq(sum(
            str_remove_all(dag$measure_nodes$label,'\\*') %in%
              colnames(dag$sig)[dag$sig[2,]==1]
          ))
        ,Y=
          dag$measure_nodes %>%
          filter(
            str_remove_all(label,'\\*') %in%
              colnames(dag$sig)[dag$sig[2,]==1]
          ) %>%
          pull(name)
        ,Z=dag$measure_nodes %>%
          filter(
            str_remove_all(label,'\\*') %in%
              colnames(dag$sig)[dag$sig[2,]==1]
          ) %>%
          pull(label)
        ,K=temp
        ,function(X,Y,Z,K){
          K=K %>%
            .[,str_detect(str_to_upper(colnames(.)),Y[X]),drop=F] %>%
            rowSums() %>%
            matrix() %>%
            `dimnames<-`(list(
              rownames(K)
              ,paste0('causal_',str_remove_all(Z[X],'\\*'))
            ))
          
          K=(K>0) %>%
            `storage.mode<-`('integer')
          
      }) %>%
      do.call(cbind,.)
    
    # Combine medical history and the causal predictors,
    # then convert frequency into probability or proportion of day being
    # encountered up to 2 years before the event
    temp=
      cbind(temp2,temp) %>%
      sweep(1,((365*2-mh_provider$day_to_event)/(365*2)),'*')
    rm(temp2)
    
    # Join the results with other attributes for data partition
    temp=
      mh_provider %>%
      select(subject_id,day_to_event,est_strata_id) %>%
      cbind(as.data.frame(temp))
    
    # Compute mean and standard deviation of age based on training set
    training_age_sum=
      readRDS('data/target_population.rds') %>%
      select(subject_id,day_to_event,outcome,age) %>%
      .[!duplicated(.),] %>%
      filter(subject_id %in% set$intv$subject_id) %>%
      summarize(age_m=mean(age),age_s=sd(age))
    
    # Get non-medical history predictors
    testing_set=
      
      ## Numerical predictors (age), standardize, and normalize into 0 to 1
      ## using the training set
      readRDS('data/target_population.rds') %>%
      select(subject_id,day_to_event,outcome,age) %>%
      .[!duplicated(.),] %>%
      filter(subject_id %in% testing_idx_db$subject_id) %>%
      cbind(training_age_sum) %>%
      mutate(age=(qnorm(0.975)+(age-age_m)/age_s)/(2*qnorm(0.975))) %>%
      
      ## Categorical predictors and getting these cleaned
      left_join(
        readRDS('data/target_population.rds') %>%
          select(
            subject_id
            ,marital_status
            ,insurance_class
            ,occupation_segment
            ,healthcare_class
          ) %>%
          .[!duplicated(.),] %>%
          group_by(subject_id) %>%
          summarize_all(function(x){
            x[!duplicated(x)] %>%
              .[which.max(
                  sapply(x[!duplicated(x)],function(x2)sum(x2==x,na.rm=T))
                )]
          }) %>%
          lapply(X=seq(ncol(.)),Y=.,function(X,Y){
            if(colnames(Y)[X] %in% c('marital_status'
                                     ,'insurance_class'
                                     ,'occupation_segment'
                                     ,'healthcare_class')){
              Y[,c('subject_id',colnames(Y)[X]),drop=F] %>%
                rename_at(colnames(Y)[X],function(x)'key') %>%
                mutate(
                  key=str_replace_all(key,'[:punct:]|[:space:]','_')
                  ,value=1
                ) %>%
                mutate_at('key',function(x)paste0(colnames(Y)[X],'.',x)) %>%
                spread(key,value) %>%
                select(-subject_id)
            }else{
              Y[,colnames(Y)[X],drop=F]
            }
          }) %>%
          do.call(cbind,.) %>%
          mutate_at(
            colnames(.) %>% .[.!='subject_id']
            ,function(x)as.integer(!is.na(x))
          )
        ,by='subject_id'
      ) %>%
      
      ## Subset internal validation set and add censoring probability
      right_join(
        temp %>%
          filter(subject_id %in% testing_idx_db$subject_id) %>%
          group_by(subject_id) %>%
          mutate(cens_p=(365*2-max(day_to_event))/(365*2)) %>%
          ungroup()
        ,by=c('subject_id','day_to_event')
      ) %>%
      
      # Convert outcome as 0 and 1
      mutate(outcome=as.integer(outcome=='event')) %>%
      
      # Pre-finishing touch
      mutate(causal_A20=age) %>%
      select(subject_id,day_to_event,est_strata_id,outcome,cens_p,everything())
    rm(temp)
    
    # Upsample minority to balance the outcome
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    testing_set %>%
      upSample(factor(.$outcome)) %>%
      mutate(
        Class=
          ifelse(Class==0,'non_event','event') %>%
          factor(c('event','non_event'))
      ) %>%
      select(
        subject_id
        ,day_to_event
        ,outcome
        ,Class
        ,age_m
        ,age_s
        ,cens_p
        ,everything()
      )
  }
  
  set$testing_geo=construct_testing_set(set$extv_geo)
  set$testing_tem=construct_testing_set(set$extv_tem)
  set$testing_bgt=construct_testing_set(set$extv_bgt)
  set$testing=
    set[c('testing_geo','testing_tem','testing_bgt')] %>%
    do.call(rbind,.)
  pred=list()
  mod_eval=list()
  
  rm(mh_provider)
}
```

```{r ... testing ... ridge regression with causal predictors, include=FALSE}
if(run_heavy_computation){
  cat('Conduct prediction and compute ROC on testing sets',append=T)
  cat('for the ridge regression with causal predictors\n')
  cat('Started:',as.character(now()),'\n')
    
    pred$ridge_calib=
      lapply(X=1:5
             ,Y=set[c(
               'training'
               ,'testing_geo'
               ,'testing_tem'
               ,'testing_bgt'
               ,'testing'
              )]
             ,Z=model
             ,K=calib_model
             ,L='ridge'
             ,function(X,Y,Z,K,L){
        M=Y[[X]] %>%
          .[,c('Class','cens_p',colnames(.) %>% .[str_detect(.,'causal')])]
        
        suppressWarnings(set.seed(33,sample.kind=sample.kind))
        data.frame(
            event=predict(Z[[L]],newdata=M,type='prob')$event
            ,obs=M$Class
          ) %>%
          mutate(calib=predict(K[[L]],newdata=.,type='prob'))
      }) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
  
    mod_eval$ridge_calib=
      pred$ridge_calib %>%
      pblapply(X=names(.),Y=.,function(X,Y){
        data.frame(Y[[X]]$calib,Y[[X]]$obs)
      }) %>%
      pblapply(evalm) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
    
  cat('End:',as.character(now()))
  saveRDS(pred$ridge_calib,'data/pred_ridge_calib.rds')
  saveRDS(mod_eval$ridge_calib,'data/mod_eval_ridge_calib.rds')
}else{
  cat(readRDS('data/log.rds')[['pred_eval_ridge_calib']])
}
```

```{r ... testing ... NPS-PC elastic net regression model, include=FALSE}
if(run_heavy_computation){
  cat('Conduct prediction and compute ROC on testing sets',append=T)
  cat('for the NPS-PC elastic net regression model\n')
  cat('Started:',as.character(now()),'\n')
    
    pred$pc_nps_elnet_calib=
      lapply(X=1:5
             ,Y=set[c(
               'training'
               ,'testing_geo'
               ,'testing_tem'
               ,'testing_bgt'
               ,'testing'
              )]
             ,Z=model
             ,K=calib_model
             ,L='pc_nps_elnet'
             ,function(X,Y,Z,K,L){
        M=select(Y[[X]],Class,cens_p) %>%
          cbind(
            pc_converter(
              Y[[X]]
              ,model$pca_nps
              ,npc=as.numeric(str_remove_all(
                  colnames(model$pc_nps_elnet$trainingData)[-1:-2]
                  ,'PC'
                ))
            )
          )
        
        suppressWarnings(set.seed(33,sample.kind=sample.kind))
        data.frame(
            event=predict(Z[[L]],newdata=M,type='prob')$event
            ,obs=M$Class
          ) %>%
          mutate(calib=predict(K[[L]],newdata=.,type='prob'))
      }) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
  
    mod_eval$pc_nps_elnet_calib=
      pred$pc_nps_elnet_calib %>%
      pblapply(X=names(.),Y=.,function(X,Y){
        data.frame(Y[[X]]$calib,Y[[X]]$obs)
      }) %>%
      pblapply(evalm) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
    
  cat('End:',as.character(now()))
  saveRDS(pred$pc_nps_elnet_calib,'data/pred_pc_nps_elnet_calib.rds')
  saveRDS(mod_eval$pc_nps_elnet_calib,'data/mod_eval_pc_nps_elnet_calib.rds')
}else{
  cat(readRDS('data/log.rds')[['pred_eval_pc_nps_elnet_calib']])
}
```

```{r ... testing ... random forest model, include=FALSE}
if(run_heavy_computation){
  cat('Conduct prediction and compute ROC on testing sets',append=T)
  cat('for the random forest model\n')
  cat('Started:',as.character(now()),'\n')
    
    pred$rf_calib=
      lapply(X=1:5
             ,Y=set[c(
               'training'
               ,'testing_geo'
               ,'testing_tem'
               ,'testing_bgt'
               ,'testing'
              )]
             ,Z=model
             ,K=calib_model
             ,L='spc_nps_rf'
             ,function(X,Y,Z,K,L){
        M=select(Y[[X]],Class,cens_p) %>%
          cbind(
            pc_converter(
              Y[[X]]
              ,model$pca_nps
              ,npc=sort(set$selected_pc$idx)
            )
          )
        
        suppressWarnings(set.seed(33,sample.kind=sample.kind))
        data.frame(
            event=predict(Z[[L]],newdata=M,type='prob')$event
            ,obs=M$Class
          ) %>%
          mutate(calib=predict(K[[L]],newdata=.,type='prob'))
      }) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
  
    mod_eval$rf_calib=
      pred$rf_calib %>%
      pblapply(X=names(.),Y=.,function(X,Y)data.frame(Y[[X]]$calib,Y[[X]]$obs)) %>%
      pblapply(evalm) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
    
  cat('End:',as.character(now()))
  saveRDS(pred$rf_calib,'data/pred_rf_calib.rds')
  saveRDS(mod_eval$rf_calib,'data/mod_eval_rf_calib.rds')
}else{
  cat(readRDS('data/log.rds')[['pred_eval_rf_calib']])
}
```

```{r ... testing ... LDA model, include=FALSE}
if(run_heavy_computation){
  cat('Conduct prediction and compute ROC on testing sets for the LDA model\n')
  cat('Started:',as.character(now()),'\n')
    
    pred$lda_calib=
      lapply(X=1:5
             ,Y=set[c(
               'training'
               ,'testing_geo'
               ,'testing_tem'
               ,'testing_bgt'
               ,'testing'
               )]
             ,Z=model
             ,K=calib_model
             ,L='spc_nps_lda'
             ,function(X,Y,Z,K,L){
        M=select(Y[[X]],Class,cens_p) %>%
          cbind(
            pc_converter(
              Y[[X]]
              ,model$pca_nps
              ,npc=sort(set$selected_pc$idx)
            )
          )
        
        suppressWarnings(set.seed(33,sample.kind=sample.kind))
        data.frame(
            event=predict(Z[[L]],newdata=M,type='prob')$event
            ,obs=M$Class
          ) %>%
          mutate(calib=predict(K[[L]],newdata=.,type='prob'))
      }) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
  
    mod_eval$lda_calib=
      pred$lda_calib %>%
      pblapply(X=names(.),Y=.,function(X,Y){
        data.frame(Y[[X]]$calib,Y[[X]]$obs)
      }) %>%
      pblapply(evalm) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
    
  cat('End:',as.character(now()))
  saveRDS(pred$lda_calib,'data/pred_lda_calib.rds')
  saveRDS(mod_eval$lda_calib,'data/mod_eval_lda_calib.rds')
}else{
  cat(readRDS('data/log.rds')[['pred_eval_lda_calib']])
}
```

```{r ... testing ... PC-GBM model, include=FALSE}
if(run_heavy_computation){
  cat('Conduct prediction and compute ROC on testing sets',append=T)
  cat('for the best model or the calibrated GBM model\n')
  cat('Started:',as.character(now()),'\n')
    
    pred$gbm_calib=
      lapply(X=1:5
             ,Y=set[c(
               'training'
               ,'testing_geo'
               ,'testing_tem'
               ,'testing_bgt'
               ,'testing'
               )]
             ,Z=model
             ,K=calib_model
             ,L='spc_nps_gbm'
             ,function(X,Y,Z,K,L){
        M=select(Y[[X]],Class,cens_p) %>%
          cbind(
            pc_converter(
              Y[[X]]
              ,model$pca_nps
              ,npc=sort(set$selected_pc$idx)
            )
          )
        
        suppressWarnings(set.seed(33,sample.kind=sample.kind))
        data.frame(
            event=predict(Z[[L]],newdata=M,type='prob')$event
            ,obs=M$Class
          ) %>%
          mutate(calib=predict(K[[L]],newdata=.,type='prob'))
      }) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
  
    mod_eval$gbm_calib=
      pred$gbm_calib %>%
      pblapply(X=names(.),Y=.,function(X,Y){
        data.frame(Y[[X]]$calib,Y[[X]]$obs)
      }) %>%
      pblapply(evalm) %>%
      setNames(c(
        'training'
        ,'testing_geo'
        ,'testing_tem'
        ,'testing_bgt'
        ,'testing'
      ))
    
  cat('End:',as.character(now()))
  saveRDS(pred$gbm_calib,'data/pred_gbm_calib.rds')
  saveRDS(mod_eval$gbm_calib,'data/mod_eval_gbm_calib.rds')
}else{
  cat(readRDS('data/log.rds')[['pred_eval_gbm_calib']])
}
```

```{r Compare testing AUROC among models, include=FALSE}
if(run_heavy_computation){
  mod_eval %>%
    .[!str_detect(names(.),'pte')] %>%
    lapply(X=names(.),Y=.,function(X,Y){
      Z=Y[[X]] %>%
        lapply(function(x)x$optres$Group1['AUC-ROC',]) %>%
        do.call(rbind,.) %>%
        rownames_to_column(var='set') %>%
        mutate(model=X)
      suppressWarnings(separate(Z,CI,c('lb','ub'),sep='-'))
    }) %>%
    do.call(rbind,.) %>%
    mutate(
      lb=ifelse(lb=='NA',Score,lb)
      ,ub=ifelse(ub=='NA',Score,ub)
    ) %>%
    mutate_at(colnames(.) %>% .[!.%in%c('model','set')],function(x)as.numeric(x)) %>%
    saveRDS('data/report_calib_test.rds')
}else{
  reported_data$calib_test=readRDS('data/report_calib_test.rds')
}
```


# Results

## Predictive performance

All models were well-calibrated after the calibration (Table 7). Although PC-RF
have a perfect calibration intercept and slope, later we will find this model
not showing evenly distributed probabilities for prediction. Following this
model, very well-calibrated models were PC-GBM, PC-LDA, and PC-LR. Causal LR
model was also well-calibrated but having higher standard deviations for both
calibration intercept and slope.

```{r echo=FALSE}
rbind(
    reported_data$pre_calislope %>% mutate(calibration='No')
    ,reported_data$post_calislope %>% mutate(calibration='Yes')
  ) %>%
  mutate(
    model=case_when(
      model=='ridge'~'1. Causal LR'
      ,model=='pc_nps_elnet'~'2. PC-LR'
      ,model=='spc_nps_rf'~'3. PC-RF'
      ,model=='spc_nps_lda'~'4. PC-LDA'
      ,model=='spc_nps_gbm'~'5. PC-GBM'
      ,TRUE~'NA'
    )
  ) %>%
  arrange(model,calibration) %>%
  select(model,calibration,everything()) %>%
  mutate(
    std.error_intercept=paste0(' ',std.error_intercept)
    ,std.error_event=paste0(' ',std.error_event)
  ) %>%
  setNames(c('Model','Calibrated','Intercept','','Slope','')) %>%
  knitr::kable(
    format='latex'
    ,caption='Calibration intercept and slope before calibration'
  ) %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

Calibration plot (Figure 4) and the probability distribution (Figure 5) are
shown. PC-RF have a perfect calibration but the probability is distributed
within  bins of 0 to <0.1 and 0.9 to <1. This is quite counter-intuitive because
there is no probabilities between 0.1 and 0.9. For causal LR, probabilities are
mostly around 0.5. This is also applied to PC-LDA but having wider distribution
up to ~0.1 and ~0.9. For PC-LR, the probability distribution is centered around
0.75. Unlike those linear models, PC-GBM have probability distribution with two
centers at 0 and 1, similar to PC-RF, but PC-GBM have probabilities between
0.1 and 0.9. These show PC-LR, PC-LDA, PC-GBM being considered as
well-calibrated based on our criteria.

```{r figure-4, echo=FALSE, fig.cap='Calibration plot before and after calibration'}
rbind(
    reported_data$pre_calib %>% mutate(plot='1. Before calibration')
    ,reported_data$post_calib %>% mutate(plot='2. After calibration')
  ) %>%
  mutate(
    model=case_when(
      model=='ridge'~'1. Causal LR'
      ,model=='pc_nps_elnet'~'2. PC-LR'
      ,model=='spc_nps_rf'~'3. PC-RF'
      ,model=='spc_nps_lda'~'4. PC-LDA'
      ,model=='spc_nps_gbm'~'5. PC-GBM'
      ,TRUE~'NA'
    )
  ) %>%
  qplot(event,obs,data=.) +
  geom_linerange(aes(ymin=lb,ymax=ub)) +
  geom_abline(lty=2) +
  facet_grid(plot~model) +
  coord_equal() +
  scale_x_continuous('Predicted probability',limits=0:1) +
  scale_y_continuous('Observed probability',limits=0:1) +
  theme(axis.text.x=element_text(angle=90,vjust=0.5,hjust=1))
```

```{r figure-5, echo=FALSE, fig.cap='Probability distribution before and after calibration'}
rbind(
    reported_data$pre_caldist %>% mutate(calibration='1. Before calibration')
    ,reported_data$post_caldist %>% mutate(calibration='2. After calibration')
  ) %>%
  mutate(
    model=case_when(
      model=='ridge'~'1. Causal LR'
      ,model=='pc_nps_elnet'~'2. PC-LR'
      ,model=='spc_nps_rf'~'3. PC-RF'
      ,model=='spc_nps_lda'~'4. PC-LDA'
      ,model=='spc_nps_gbm'~'5. PC-GBM'
      ,TRUE~'NA'
    )
  ) %>%
  group_by(model) %>%
  mutate(n=n/max(n)) %>%
  ungroup() %>%
  qplot(event,n,data=.,geom='col',na.rm=T) +
  facet_grid(calibration~model,scales='free_y') +
  scale_x_continuous('Percentile') +
  scale_y_continuous('Frequency per max. freq. each model') +
  theme(axis.text.x=element_text(angle=90,vjust=0.5,hjust=1))
```

All AUROCs before and after calibration are shown (Figure 6). The predictive
performances were evaluated using internal validation set. PC-GBM outperforms
other well-calibrated models which were PC-LDA and PC-LR. Therefore, we chose
PC-GBM as the best model.

```{r figure-6, echo=FALSE, fig.cap='AUROC plot among models before and after calibration'}
rbind(
    reported_data$pre_roc %>% mutate(calibration='No')
    ,reported_data$post_roc %>% mutate(calibration='Yes')
  ) %>%
  mutate(
    model=case_when(
      model=='ridge'~'Causal LR'
      ,model=='pc_nps_elnet'~'PC-LR'
      ,model=='spc_nps_rf'~'PC-RF'
      ,model=='spc_nps_lda'~'PC-LDA'
      ,model=='spc_nps_gbm'~'PC-GBM'
      ,TRUE~'NA'
    )
  ) %>%
  mutate(
    calibration=factor(calibration,rev(unique(calibration)))
    ,model=reorder(model,ROC,mean)
  ) %>%
  qplot(calibration,ROC,data=.) +
  geom_errorbar(aes(ymin=ROC_lb,ymax=ROC_ub),width=0.35) +
  facet_grid(model~.,scales='free_y') +
  coord_flip() +
  scale_x_discrete('Calibrated') +
  scale_y_continuous('AUROC (95% CI)') +
  theme(strip.text.y=element_text(angle=0))
```

The best model achieved AUROC of 0.98 (95% CI 0.98 to 0.98) (Table 8). Because
the sample size of internal validation set was very large, then we have a very
narrow interval estimates and AUROC number. The AUROC of PC-RF is not exactly 1,
but just being rounded to this number.

```{r echo=FALSE}
rbind(
    reported_data$pre_auroc %>% mutate(calibration='No')
    ,reported_data$post_auroc %>% mutate(calibration='Yes')
  ) %>%
  mutate(
    model=case_when(
      model=='ridge'~'Causal LR'
      ,model=='pc_nps_elnet'~'PC-LR'
      ,model=='spc_nps_rf'~'PC-RF'
      ,model=='spc_nps_lda'~'PC-LDA'
      ,model=='spc_nps_gbm'~'PC-GBM'
      ,TRUE~'NA'
    )
  ) %>%
  mutate(
    calibration=factor(calibration,unique(calibration))
    ,model=reorder(model,ROC,mean)
  ) %>%
  arrange(model,calibration) %>%
  mutate_at(c('ROC','ROC_lb','ROC_ub'),function(x)round(x,2)) %>%
  mutate(lb_ub=paste0(ROC_lb,' to ',ROC_ub)) %>%
  select(model,calibration,ROC,lb_ub) %>%
  setNames(c('Model','Calibrated','AUROC','95% CI')) %>%
  kable(
    format='latex'
    ,caption='Compare AUROC among models before and after calibration'
  ) %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

After choosing the best model, we need to validate the predictive performance.
We found that PC-LR is slightly better than PC-GBM in aggregated external
validation set (Figure 7). This may happen by chance. We stick to choose PC-GBM
as the best model to avoid overfitting. Nevertheless, of three splits for
external validation sets, PC-LR was not always better than PC-GBM. Therefore,
PC-GBM may still be the best prediction model for future/unobserved data.

```{r figure-7, echo=FALSE, fig.cap='Plot AUROC among models'}
reported_data$calib_test %>%
  mutate(
    model=case_when(
      model=='ridge_calib'~'Causal LR'
      ,model=='pc_nps_elnet_calib'~'PC-LR'
      ,model=='rf_calib'~'PC-RF'
      ,model=='lda_calib'~'PC-LDA'
      ,model=='gbm_calib'~'PC-GBM'
      ,TRUE~'NA'
    )
    ,set=case_when(
      set=='training'~'Internal validation'
      ,set=='testing'~'External validation'
      ,set=='testing_geo'~'External validation, geographical split'
      ,set=='testing_tem'~'External validation, temporal split'
      ,set=='testing_bgt'~'External validation, geotemporal split'
      ,TRUE~'NA'
    )
  ) %>%
  mutate(
    model=
      factor(
        model
        ,c('Causal LR'
           ,'PC-LDA'
           ,'PC-LR'
           ,'PC-GBM'
           ,'PC-RF') %>% rev()
      )
    ,set=
      factor(
        set
        ,c('Internal validation'
           ,paste0('External validation',c(''
                                           ,', geographical split'
                                           ,', temporal split'
                                           ,', geotemporal split')))
      )
  ) %>%
  mutate(best_model=ifelse(model=='PC-GBM','Yes','No') %>% factor()) %>%
  qplot(model,Score,color=best_model,data=.) +
  geom_errorbar(aes(ymin=lb,ymax=ub)) +
  facet_grid(set~.) +
  coord_flip() +
  scale_x_discrete('Model') +
  scale_y_continuous('AUROC (95% CI)') +
  scale_color_discrete(
    'The best well-calibrated model based on internal validation'
  ) +
  theme(strip.text.y=element_text(angle=0),legend.position='top')
```

However, we found that PC-LR have a very low standard deviation of AUROCs among
any sets (Figure 7 and Table 9). This reflects a very consistent predictive
performance. Nevertheless, this is reasonable for linear models. Unlike these
models, tree-ensemble models such RF and GBM improve predictive performances
by overfitting the data. Yet, we considered these models likely to be selected
using internal validation set, especially if the predictive performance is
almost perfect (e.g. AUROC >0.98). This situation makes other models almost
impossible to outperform RF and GBM using internal validation.

```{r echo=FALSE}
reported_data$calib_test %>%
  mutate(
    model=case_when(
      model=='ridge_calib'~'Causal LR'
      ,model=='pc_nps_elnet_calib'~'PC-LR'
      ,model=='rf_calib'~'PC-RF'
      ,model=='lda_calib'~'PC-LDA'
      ,model=='gbm_calib'~'PC-GBM'
      ,TRUE~'NA'
    )
    ,set=case_when(
      set=='training'~'Int. validation'
      ,set=='testing'~'Ext. validation'
      ,set=='testing_geo'~'Ext. validation, geographical split'
      ,set=='testing_tem'~'Ext. validation, temporal split'
      ,set=='testing_bgt'~'Ext. validation, geotemporal split'
      ,TRUE~'NA'
    )
  ) %>%
  mutate(
    model=
      factor(
        model
        ,c('Causal LR'
           ,'PC-LDA'
           ,'PC-LR'
           ,'PC-GBM'
           ,'PC-RF')
      )
    ,set=
      factor(
        set
        ,c('Int. validation'
           ,paste0('Ext. validation',c(''
                                       ,', geographical split'
                                       ,', temporal split'
                                       ,', geotemporal split')))
      )
  ) %>%
  arrange(model,set) %>%
  group_by(model) %>%
  mutate(
    auroc_sd=sd(Score) %>% round(3)
    ,lb_ub=paste0(lb,' to ',ub)
  ) %>%
  select(model,set,Score,lb_ub,auroc_sd) %>%
  setNames(c('Model','Validation set','AUROC','95% CI','SD among sets')) %>%
  kable(
    format='latex'
    ,caption='Compare AUROC among models'
  ) %>%
  kable_styling(full_width=T,latex_options='HOLD_position')
```

## Interpretation of the best model

We used predictor weight in every PC and the PC importance in PC-GBM model to
rank predictor importance. Then, we focus on predictors that were causal factors
(Figure 8). Maternal age is considered the most important among these factors.
This possibly because age is confounding to many predictors; thus, PC-GBM may
exploit the confounding path through this factor to explain data variation
for predicting the outcome. Interestingly, by the rank, causal factors from
intraamniotic infection (IAI) to pneumonia belong to a common path (Figure 3).
We may consider this path as infectious/immune disease path. Meanwhile,
antepartum hemorrhage (APH) and assisted reproduction belong to another common
path. But, by prevalence, assisted reproduction with APH was less common
compared to the conditions in the infectious/immune disease path.

```{r Get rotated PCs as the PC weights, include=FALSE}
if(run_heavy_computation){
  rotated_pc=
    
    # Subset the top PCs and stack all versions of PCs
    model$pca_nps %>%
    lapply(X=seq(length(.)),Y=.,function(X,Y){
      as.data.frame(Y[[X]]$prcomp$rotation[,sort(set$selected_pc$idx)]) %>%
        rownames_to_column(var='predictor')
    }) %>%
    do.call(rbind,.) %>%
    
    # Group by predictor and average the PC weights over all versions
    group_by(predictor) %>%
    summarize_all(function(x)mean(x,na.rm=T)) %>%
    ungroup() %>%
    column_to_rownames(var='predictor')
  
  saveRDS(rotated_pc,'data/rotated_pc.rds')
}else{
  rotated_pc=readRDS('data/rotated_pc.rds')
}
```

```{r Get variable importance of PC-GBM, include=FALSE}
if(run_heavy_computation){
  varimp_gbm=
    model$spc_nps_gbm %>%
    varImp() %>%
    .$importance %>%
    rownames_to_column(var='feature') %>%
    rename(importance=Overall)
  
  saveRDS(varimp_gbm,'data/varimp_gbm.rds')
}else{
  varimp_gbm=readRDS('data/varimp_gbm.rds')
}
```

```{r figure-8, echo=FALSE, fig.cap='Causal factors by PC-GBM predictor importance'}
rotated_pc %>%
  rownames_to_column(var='predictor') %>%
  gather(feature,weight,-predictor) %>%
  left_join(varimp_gbm,by='feature') %>%
  mutate(
    feature=reorder(feature,importance)
    ,w_imp=abs(weight)*importance
  ) %>%
  group_by(predictor) %>%
  summarize(sum_w_imp=sum(w_imp)) %>%
  arrange(desc(sum_w_imp)) %>%
  ungroup() %>%
  mutate(rank=seq(nrow(.))) %>%
  filter(str_detect(predictor,'causal')) %>%
  mutate(predictor=str_remove_all(predictor,'causal_')) %>%
  left_join(rename(dag$baseline_nodes,predictor=label),by='predictor') %>%
  mutate(predictor=paste0(predictor,' - ',name)) %>%
  mutate(predictor=reorder(predictor,sum_w_imp)) %>%
  qplot(predictor,sum_w_imp,data=.,geom='col') +
  geom_text(aes(label=rank),hjust=2) +
  coord_flip() +
  scale_x_discrete('Causal factors and the rank') +
  scale_y_continuous('Total PCs sum of PC-wise weight  PC importance') +
  theme(strip.text.y=element_text(angle=0))
```

By this finding, we consider the outcome possibly caused by influenza but
indirectly by modifying immune responses. These may disrupt normal microbial
community within uterus. Although uterus is believed as a sterile organ for many
years, recent studies show normal flora (microbial community) existing in this
organ. Therefore, change in uterine normal flora may affect the outcome
predicted by the PC-GBM model.

# Conclusions

## Brief summary
We have developed a prediction model for a pregnancy outcome. This model applied
gradient boosting machine algorithm and used PCs of causal factors, medical
history, and demographical variables. The model achieved 0.69 (95% CI 0.68 to
0.70) based on external validation. Influenza as the cause of the outcome was an
insight gained from its rank of importance within the PC-GBM model.

## Potential impact
Only one previous study have developed a prediction model for this outcome.
The AUROC was 0.67 using internal validation, while our model achieved 0.98
using internal validation. This model may help improving prevention of the
outcome by prediction and early intervention. Since no prevention method is
already available for the outcome, insight from our prediction model may help
to develop the prevention strategy, e.g. by identifying the timing based on
the best one for prediction.

## Limitations
Our model is still need to improve for achieving clinically-significant
predictive performance using external validation. Although we can interpret the
variable importance to get new insight from our prediction model, we still
cannot interpret how predictor-to-predictor relationship to describe the
pathogenesis model of the outcome.

We also did not have data for several causal/confounding factors. Although
G-estimation is found to be robust even using an incorrectly-specified causal
model, we still cannot block potential confounding since lacking of data.

## Future work
Molecular studies are needed to develop both prediction and prevention for the
outcome. We will use omics data to pursue these goals. A prediction model using
biomarkers may improve the predictive performance. Our model in this study
can be used as a preliminary prediction model to reduce healthcare cost due
to the biomarker tests.

Understanding to the molecular mechanism of the outcome may help on the
prevention strategy development. It should be relevant to the prediction models;
thus, we may expect an integrated prediction-prevention strategy to improve this
pregnancy outcome.
